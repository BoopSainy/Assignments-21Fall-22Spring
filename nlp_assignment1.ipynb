{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM6513] Assignment 1: Sentiment Analysis with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test a **text classification** system for **sentiment analysis**, in particular to predict the sentiment of movie reviews, i.e. positive or negative (binary classification).\n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using \n",
    "    - n-grams (BOW), i.e. unigrams, bigrams and trigrams to obtain vector representations of documents where n=1,2,3 respectively. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). \n",
    "    - character n-grams (BOCN). A character n-gram is a contiguous sequence of characters given a word, e.g. for n=2, 'coffee' is split into {'co', 'of', 'ff', 'fe', 'ee'}. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). **Tip: Note the large vocabulary size!** \n",
    "    - a combination of the two vector spaces (n-grams and character n-grams) choosing your best performing wighting respectively (i.e. raw or tfidf). (**1 mark**) **Tip: you should merge the two representations**\n",
    "\n",
    "\n",
    "\n",
    "- Binary Logistic Regression (LR) classifiers that will be able to accurately classify movie reviews trained with: \n",
    "    - (1) BOW-count (raw frequencies) \n",
    "    - (2) BOW-tfidf (tf.idf weighted)\n",
    "    - (3) BOCN-count\n",
    "    - (4) BOCN-tfidf\n",
    "    - (5) BOW+BOCN (best performing weighting; raw or tfidf)\n",
    "\n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function (**1 mark**)\n",
    "    - Use L2 regularisation (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous development loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength) for each LR model? You should use a table showing model performance using different set of hyperparameter values. (**2 marks). **Tip: Instead of using all possible combinations, you could perform a random sampling of combinations.**\n",
    "\n",
    "\n",
    "- After training each LR model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot. Does your model underfit, overfit or is it about right? Explain why. (**1 mark**). \n",
    "\n",
    "\n",
    "- Identify and show the most important features (model interpretability) for each class (i.e. top-10 most positive and top-10 negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!). If you were to apply the classifier into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks**)\n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices (**2 marks**). \n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible (you can find tips in Lab 1 sheet). Executing the whole notebook with your code should not take more than 5 minutes on a any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex` or you can print it as PDF using your browser).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 14 Mar 2022** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T13:41:17.642162Z",
     "start_time": "2020-03-27T13:41:16.891940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the whole running time except the five hyperparameters tunning parts is about 130-140s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [],
   "source": [
    "# use Pandas to load original training, development, and test datasets from 'csv' file.\n",
    "\n",
    "train_ori = pd.read_csv('data_sentiment/train.csv', header = None)\n",
    "dev_ori = pd.read_csv('data_sentiment/dev.csv', header = None)\n",
    "test_ori = pd.read_csv('data_sentiment/test.csv', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.900892Z",
     "start_time": "2020-02-15T14:17:28.891221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check first five samples from training dataset\n",
      "                                                   0  1\n",
      "0  note : some may consider portions of the follo...  1\n",
      "1  note : some may consider portions of the follo...  1\n",
      "2  every once in a while you see a film that is s...  1\n",
      "3  when i was growing up in 1970s , boys in my sc...  1\n",
      "4  the muppet movie is the first , and the best m...  1\n",
      "\n",
      "\n",
      "check first five samples from development dataset\n",
      "                                                   0  1\n",
      "0  if he doesn=92t watch out , mel gibson is in d...  1\n",
      "1  wong kar-wei's \" fallen angels \" is , on a pur...  1\n",
      "2  there is nothing like american history x in th...  1\n",
      "3  an unhappy italian housewife , a lonely waiter...  1\n",
      "4  when people are talking about good old times ,...  1\n",
      "\n",
      "\n",
      "check first five samples from test dataset\n",
      "                                                   0  1\n",
      "0  wild things is a suspenseful thriller starring...  1\n",
      "1  i know it already opened in december , but i f...  1\n",
      "2  what's shocking about \" carlito's way \" is how...  1\n",
      "3  uncompromising french director robert bresson'...  1\n",
      "4  aggressive , bleak , and unrelenting film abou...  1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the loaded data\n",
    "\n",
    "print('check first five samples from training dataset')\n",
    "print(train_ori.iloc[0:5])\n",
    "print('\\n')\n",
    "\n",
    "print('check first five samples from development dataset')\n",
    "print(dev_ori.iloc[0:5])\n",
    "print('\\n')\n",
    "\n",
    "print('check first five samples from test dataset')\n",
    "print(test_ori.iloc[0:5])\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "# put the raw texts into list and corresponding labels into NumPy arrays\n",
    "\n",
    "train_text = [train_ori.iloc[i, 0] for i in range(train_ori.shape[0])]\n",
    "train_label = np.array([train_ori.iloc[i, 1] for i in range(train_ori.shape[0])]) # train_label's shape is (1400, )\n",
    "\n",
    "dev_text = [dev_ori.iloc[i, 0] for i in range(dev_ori.shape[0])]\n",
    "dev_label = np.array([dev_ori.iloc[i, 1] for i in range(dev_ori.shape[0])]) # dev_label's shape is (200, )\n",
    "\n",
    "test_text = [test_ori.iloc[i, 0] for i in range(test_ori.shape[0])]\n",
    "test_label = np.array([test_ori.iloc[i, 1] for i in range(test_ori.shape[0])]) # test_label's shape is (400, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations of Text \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams (or character ngrams from the unigrams)\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (or character n-grams). You can keep top N if you encounter memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "# I add 'am', 'll', 'also', 'why', 'would', 'should', 'their', 'might', 'there', 'where', 'him' to the \n",
    "# stop_words list, because these words are not useful to sentiment classification task.\n",
    "\n",
    "stop_words = ['a','am','in','ll','on','at','and','or', \"also\", \"why\", \"would\", \"should\", \"their\", \"might\",\n",
    "              'to', 'the', 'of', 'an', 'by', \"there\", \"where\", \"him\",\n",
    "              'as', 'is', 'was', 'were', 'been', 'be',\n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `char_ngrams`: boolean. If true the function extracts character n-grams\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `x': a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.169090Z",
     "start_time": "2020-02-15T14:17:33.161268Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'', \n",
    "                   stop_words=[], vocab=set(), char_ngrams=False):\n",
    "    \"\"\"\n",
    "        We are going to convert the tokens to lowercase, removes the stopwords and punctuations before \n",
    "        generating bigrams or trigrams.\n",
    "    \"\"\"\n",
    "    # The original texts has been converted to lowercase\n",
    "    \n",
    "    # remove all punctuations, remove all numbers\n",
    "    # find those words containing two or more than two letters, remove simple words such as \"a\", \"I\"...\n",
    "    # drop all numbers\n",
    "    # list_word = re.split(r'[^\\w]+', x_raw) \n",
    "    \n",
    "    list_word = re.findall(r\"(?u)\\b\\w\\w+\\b\", x_raw) \n",
    "        \n",
    "    # Remove the stopwords\n",
    "    for word in stop_words:\n",
    "        occ = list_word.count(word)\n",
    "        for i in range(occ):\n",
    "            list_word.remove(word)\n",
    "    \n",
    "    #     Get the n_grams features for x_raw based on flag char_ngrams, if char_ngrams is False, then pick up words \n",
    "    # features. If char_ngrams is True, then pick up characters features.\n",
    "    \n",
    "    if char_ngrams == False:\n",
    "        list_ngrams = [] # initialize the list of ngrams\n",
    "        \n",
    "        # obtain unigrams, bigrams, or trigrams according to argument - 'ngram_range'.\n",
    "        for i in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            if i == 1: # get the list of unigrams for x_raw\n",
    "                list_unigrams = list_word\n",
    "                list_ngrams += list_unigrams\n",
    "            elif i == 2:\n",
    "                list_bigrams = []\n",
    "                for j in range(len(list_word) - 1):\n",
    "                    list_bigrams.append(list_word[j] + ' ' + list_word[j + 1])\n",
    "                    \n",
    "                list_ngrams += list_bigrams\n",
    "            else:\n",
    "                list_trigrams = []\n",
    "                for j in range(len(list_word) - 2):\n",
    "                    list_trigrams.append(list_word[j] + ' ' + list_word[j + 1] \n",
    "                                         + ' ' + list_word[j + 2])\n",
    "                \n",
    "                list_ngrams += list_trigrams\n",
    "    \n",
    "        list_extracted_feature = list_ngrams\n",
    "    \n",
    "    # Get the char_n_grams features\n",
    "    \n",
    "    else:\n",
    "        list_char_ngrams = [] # initialize the list of character ngrams\n",
    "        whole_string = \"\".join(list_word)\n",
    "        \n",
    "        # obtain char-unigrams, char-bigrams, or char-bigrams according to argument - \n",
    "        # 'ngram_range'\n",
    "        for i in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            list_char_temp = []\n",
    "            for j in range(len(whole_string) - i + 1):\n",
    "                list_char_temp.append(whole_string[j:j+i])\n",
    "            list_char_ngrams += list_char_temp\n",
    "        \n",
    "        list_extracted_feature = list_char_ngrams\n",
    "    \n",
    "    # Get specific features if 'vocab' argument is given\n",
    "    \n",
    "    if len(vocab) != 0:\n",
    "        list_specific_feature = []\n",
    "        for feature in list_extracted_feature:\n",
    "            if feature in vocab:\n",
    "                list_specific_feature.append(feature)\n",
    "        \n",
    "        list_extracted_feature = list_specific_feature\n",
    "        \n",
    "    \n",
    "    return list_extracted_feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting character n-grams the function should work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Son',\n",
       " 'ong',\n",
       " 'ngy',\n",
       " 'gya',\n",
       " 'yan',\n",
       " 'ang',\n",
       " 'ngL',\n",
       " 'gLi',\n",
       " 'Song',\n",
       " 'ongy',\n",
       " 'ngya',\n",
       " 'gyan',\n",
       " 'yang',\n",
       " 'angL',\n",
       " 'ngLi',\n",
       " 'Songy',\n",
       " 'ongya',\n",
       " 'ngyan',\n",
       " 'gyang',\n",
       " 'yangL',\n",
       " 'angLi',\n",
       " 'Songya',\n",
       " 'ongyan',\n",
       " 'ngyang',\n",
       " 'gyangL',\n",
       " 'yangLi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the extract_ngrams function.\n",
    "extract_ngrams('Song yang Li', \n",
    "               ngram_range=(3, 6), \n",
    "               stop_words=stop_words,\n",
    "               char_ngrams=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary \n",
    "\n",
    "The `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:35.821240Z",
     "start_time": "2020-02-15T14:17:35.814722Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'', \n",
    "              min_df=0, keep_topN=None, \n",
    "              stop_words=[],char_ngrams=False):\n",
    "    \n",
    "    # initialize a set to contain ngrams that will be used as features\n",
    "    vocab = set()\n",
    "    \n",
    "    # initialize the dictionary to contain ngrams as keys and their corresponding document\n",
    "    # frequency as values\n",
    "    df = dict()\n",
    "    \n",
    "    # initialize the dictionary to contain ngrams as keys and their corresponding counts in\n",
    "    # vocab as values\n",
    "    ngram_counts = dict()\n",
    "    \n",
    "    # extract features for each documents in X_raw\n",
    "    for x_raw in X_raw:\n",
    "        extracted_feature = extract_ngrams(x_raw = x_raw, ngram_range = ngram_range, \n",
    "                                          stop_words = stop_words, char_ngrams = char_ngrams)\n",
    "        \n",
    "        # get the dictionary for term frequency\n",
    "        for feature in extracted_feature:\n",
    "            if feature not in ngram_counts:\n",
    "                ngram_counts[feature] = 1\n",
    "            else:\n",
    "                ngram_counts[feature] += 1\n",
    "        \n",
    "        # get the dictionary for document frequency\n",
    "        for feature in set(extracted_feature):\n",
    "            if feature not in df:\n",
    "                df[feature] = 1\n",
    "            else:\n",
    "                df[feature] += 1\n",
    "                \n",
    "                \n",
    "    \n",
    "    \"\"\"\n",
    "            When build the vocabulary, we need to consider two arguments \"keep_topN\" and \"min_df\".\n",
    "            If \"keep_topN\" is None, then we only need to consider the condition of \"min_df\". We will ignore those \n",
    "        words with document frequencies lower than \"min_df\".\n",
    "            However, it \"keep_topN\" is not None but an integer. Then we should also consider the value of \"min_df\".\n",
    "        In this case, we will keep top \"keep_topN\" words based on their frequencies in the whole collection, \n",
    "        and meanwhile, these words' document frequencies should be higher than \"min_df\".\n",
    "    \"\"\"\n",
    "    if keep_topN == None:\n",
    "        if min_df == 0:\n",
    "            for feature in df:\n",
    "                vocab.add(feature)\n",
    "        else:\n",
    "            for feature in df:\n",
    "                if df[feature] >= min_df:\n",
    "                    vocab.add(feature)\n",
    "    else:\n",
    "        # sort the ngrams based on their corresponding counts, from greater to lower.\n",
    "        sorted_ngram_counts = dict(sorted(ngram_counts.items(),\n",
    "                                         key = lambda item: item[1], reverse = True))\n",
    "        sorted_ngram_counts = list(sorted_ngram_counts.keys())\n",
    "        \n",
    "        if min_df == 0:\n",
    "            for i in range(keep_topN):\n",
    "                vocab.add(sorted_ngram_counts[i])\n",
    "        else:\n",
    "            i = 0\n",
    "            while(len(vocab) < keep_topN):\n",
    "                if df[sorted_ngram_counts[i]] >= min_df:\n",
    "                    vocab.add(sorted_ngram_counts[i])\n",
    "                i += 1\n",
    "    \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.319793Z",
     "start_time": "2020-02-15T14:17:36.836545Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a vocabulary, and get document and raw frequencies of n-grams.\n",
    "# set I need unigrams, bigrams, and trigrams with a document frequencies higher than 20\n",
    "\n",
    "vocab_bow, df_bow, ngram_counts_bow = get_vocab(X_raw = train_text, ngram_range = (1, 3), min_df = 20, \n",
    "                                    stop_words = stop_words, char_ngrams = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create 2 dictionaries: (1) vocabulary id -> word; and  (2) word -> vocabulary id so you can use them for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [],
   "source": [
    "#     create 2 dictionaries: (1) id2word_bow: keys = vocabulary ids, values = words (2) word2id_bow: keys = words, \n",
    "# values = vocabulary ids\n",
    "\n",
    "id2word_bow = dict()\n",
    "word2id_bow = dict()\n",
    "\n",
    "for i in range(len(vocab_bow)):\n",
    "    id2word_bow[i] = list(vocab_bow)[i]\n",
    "    word2id_bow[list(vocab_bow)[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.213253Z",
     "start_time": "2020-02-15T14:17:39.329147Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize three list for training texts, development texts, and test texts\n",
    "train_extracted_ngrams = []\n",
    "dev_extracted_ngrams = []\n",
    "test_extracted_ngrams = []\n",
    "\n",
    "# extract ngrams for training set\n",
    "for i in range(len(train_text)):\n",
    "    temp = extract_ngrams(x_raw = train_text[i], ngram_range = (1, 3), \n",
    "                          stop_words = stop_words, char_ngrams = False)\n",
    "    train_extracted_ngrams.append(temp)\n",
    "\n",
    "# extract ngrams for development set\n",
    "for i in range(len(dev_text)):\n",
    "    temp = extract_ngrams(x_raw = dev_text[i], ngram_range = (1, 3),\n",
    "                         stop_words = stop_words, char_ngrams = False)\n",
    "    dev_extracted_ngrams.append(temp)\n",
    "\n",
    "# extract ngrams for test set\n",
    "for i in range(len(test_text)):\n",
    "    temp = extract_ngrams(x_raw = test_text[i], ngram_range = (1, 3),\n",
    "                         stop_words = stop_words, char_ngrams = False)\n",
    "    test_extracted_ngrams.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function could vectorise word/char features of training documents to a feature value matrix.\n",
    "def vectorise(X_ngram, vocab, word2id):\n",
    "    # initialize a N x |vocab| array for X_vec\n",
    "    X_vec = np.zeros((len(X_ngram), len(vocab)))\n",
    "    \n",
    "    row = 0\n",
    "    for doc in X_ngram:\n",
    "        for ngram in doc:\n",
    "            if ngram in vocab:\n",
    "                X_vec[row, word2id[ngram]] += 1\n",
    "        \n",
    "        row += 1\n",
    "    \n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:41.999574Z",
     "start_time": "2020-02-15T14:17:40.376534Z"
    }
   },
   "outputs": [],
   "source": [
    "# obtain count vectors for each document in the train, developemnt and test set\n",
    "train_count_mat = vectorise(train_extracted_ngrams, vocab_bow, word2id_bow)\n",
    "dev_count_mat = vectorise(dev_extracted_ngrams, vocab_bow, word2id_bow)\n",
    "test_count_mat = vectorise(test_extracted_ngrams, vocab_bow, word2id_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.022692Z",
     "start_time": "2020-02-15T14:17:42.012315Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the inverted document frequencies values for words in vocab\n",
    "idfs_bow = np.zeros((1, len(vocab_bow)))\n",
    "\n",
    "for i in range(len(vocab_bow)):\n",
    "    idfs_bow[0, i] = np.log10(len(train_text)/df_bow[list(vocab_bow)[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.802265Z",
     "start_time": "2020-02-15T14:17:42.752448Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform the count vectors to tf.idf vectors\n",
    "\n",
    "train_tfidf_mat = train_count_mat * idfs_bow\n",
    "dev_tfidf_mat = dev_count_mat * idfs_bow\n",
    "test_tfidf_mat = test_count_mat * idfs_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # sigmoid of z\n",
    "    \n",
    "    sig = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "# function predict_proba will return a array with size N*1, where N is the documents number of input data.\n",
    "\n",
    "def predict_proba(X, weights):\n",
    "    \n",
    "    # weights should be reshaped to |vocab|*1 size\n",
    "    \n",
    "    weights = weights.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    weighted_input = X @ weights\n",
    "    \n",
    "    preds_proba = sigmoid(weighted_input)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    preds_proba = predict_proba(X, weights) # a N*1 shape array\n",
    "    \n",
    "    for row in range(preds_proba.shape[0]):\n",
    "        if (preds_proba[row, 0] >= 0.5):\n",
    "            preds_proba[row, 0] = 1\n",
    "        else:\n",
    "            preds_proba[row, 0] = 0\n",
    "    \n",
    "    preds_class = preds_proba\n",
    "    \n",
    "    \n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    '''\n",
    "    Binary Cross-entropy Loss\n",
    "\n",
    "    X:(len(X),len(vocab))\n",
    "    Y: array len(Y)\n",
    "    weights: array len(X)\n",
    "    '''\n",
    "    \n",
    "    # initialize the loss\n",
    "    l = 0\n",
    "    \n",
    "    # compute the binary entropy loss for all documents in X\n",
    "    \n",
    "    pred_proba = predict_proba(X, weights)\n",
    "    \n",
    "    # l = -Y*(log(P)) - (1 - Y)*log(1 - P) + regularization term\n",
    "    \n",
    "    l = -Y.reshape(1, -1) @ np.log(1e-15 + pred_proba) - \\\n",
    "        (1 - Y.reshape(1, -1)) @ np.log(1e-15 + 1 - pred_proba) + \\\n",
    "        1/2*alpha*(weights.reshape(1, -1) @ weights.reshape(-1, 1))\n",
    "    \n",
    "    l = l/X.shape[0]\n",
    "\n",
    "    return l\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.968510Z",
     "start_time": "2020-02-15T14:17:45.958185Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], lr=0.1, \n",
    "        alpha=0.00001, epochs=5, \n",
    "        tolerance=0.0001, print_progress=True):\n",
    "    \n",
    "        \n",
    "    # initialize the weights\n",
    "    weights = np.zeros((X_tr.shape[1], 1)) # size: |vocab|*1\n",
    "    \n",
    "    # merge the features and labels of training set and dev set into lists by zip(), it will be useful to \n",
    "    # randomize their orders before each epoch\n",
    "    \n",
    "    tr = list(zip(X_tr, Y_tr))\n",
    "    \n",
    "    # initialize training_loss_history and validation_loss_history\n",
    "    training_loss_history = []\n",
    "    training_loss_history.append(binary_loss(X_tr, Y_tr, weights, alpha = alpha)[0, 0])\n",
    "    validation_loss_history = []\n",
    "    validation_loss_history.append(binary_loss(X_dev, Y_dev, weights, alpha = alpha)[0, 0])\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # randomise the training set at the beginning of each epoch\n",
    "        random.shuffle(tr)\n",
    "        \n",
    "        for x, y in tr:\n",
    "            # update the weights by following equation\n",
    "            \"\"\"\n",
    "            new_weights = old_weights - (learning rate) * (((p - y) * x) + (regularization term))  \n",
    "            \"\"\"\n",
    "            \n",
    "            weights = weights - lr*(((predict_proba(x, weights) - y)*x).reshape(-1, 1) + alpha*weights)\n",
    "            \n",
    "        \n",
    "        # update the training and validation loss by using updated weights\n",
    "        training_loss_history.append(binary_loss(X_tr, Y_tr, weights, alpha = alpha)[0, 0])\n",
    "        validation_loss_history.append(binary_loss(X_dev, Y_dev, weights, alpha = alpha)[0, 0])\n",
    "        \n",
    "        # print the training and validation loss if print_progress flag is True\n",
    "        if print_progress == True:\n",
    "            print(\"epoch {:3d}: training loss is {:.6f}, validation loss is {:.6f}.\"\\\n",
    "                  .format(epoch, \n",
    "                          training_loss_history[epoch + 1], \n",
    "                          validation_loss_history[epoch + 1]))\n",
    "        \n",
    "        # I use the absolute value of difference between previous validation loss and current validation loss.\n",
    "        # Because there might be slight rise and fall in validation loss in training period, but the overall tendency\n",
    "        # of validation loss is still falling.\n",
    "        \n",
    "        if abs(validation_loss_history[epoch] - validation_loss_history[epoch + 1]) <= tolerance:\n",
    "            break\n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: BOW-COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:\n",
    "\n",
    "Define a function to choose the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By comparing the validation loss under different hyperparameter combinations, we could choose the best \n",
    "# hyperparameters with the lowest validation loss.\n",
    "\n",
    "# I choose to define a function to choose the hyperparameters\n",
    "\n",
    "def choose_hyper(X_tr, Y_tr, X_dev, Y_dev, lr_candidate = [], alpha_candidate = [], epoch_candidate = [], \n",
    "                 tolerance_candidate = []):\n",
    "    \n",
    "    \"\"\"\n",
    "    The arguments lr_candidate, alpha_candidate, epoch_candidate, and tolerance_candidate are the candidates for \n",
    "    the best hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # initialize a validation loss\n",
    "    current_val_loss = 100000\n",
    "    \n",
    "    # compute different validation loss and validation accuracy under different hyperparameter combinations\n",
    "    \n",
    "    for lr in lr_candidate:\n",
    "        for alpha in alpha_candidate:\n",
    "            for epoch in epoch_candidate:\n",
    "                for tolerance in tolerance_candidate:\n",
    "                    weights, tr_loss, val_loss = SGD(X_tr, Y_tr, X_dev, Y_dev, lr = lr, alpha = alpha, \n",
    "                                                     epochs = epoch, tolerance = tolerance, print_progress = False)\n",
    "                    \n",
    "                    print(\"lr = {:.6f}, alpha = {:.6f}, epoch = {:3d}, tolerance = {:.6f}, val_loss = {:.6f}, val_acc = {:.6f}\"\\\n",
    "                         .format(lr, alpha, epoch, tolerance, val_loss[-1], \n",
    "                                 accuracy_score(Y_dev, predict_class(X_dev, weights))))\n",
    "                    \n",
    "                    print(\"\")\n",
    "                    \n",
    "                    if val_loss[-1] < current_val_loss:\n",
    "                        current_val_loss = val_loss[-1]\n",
    "                        best_lr = lr\n",
    "                        best_alpha = alpha\n",
    "                        best_epoch = epoch\n",
    "                        best_tolerance = tolerance\n",
    "    \n",
    "    \n",
    "    # return the best hyperparameters which could reach the best validation loss\n",
    "    best_parameter = [best_lr, best_alpha, best_epoch, best_tolerance]\n",
    "    \n",
    "    print(\"The best parameters are: lr = {}, alpha = {}, epoch = {}, tolerance = {}\".format(best_parameter[0], \n",
    "                                                                                           best_parameter[1],\n",
    "                                                                                           best_parameter[2],\n",
    "                                                                                           best_parameter[3]))\n",
    "    \n",
    "\n",
    "    \n",
    "    return best_parameter\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning for BOW-COUNT model. \n",
    "\n",
    "This part is not counted in time for grading.\n",
    "\n",
    "Choose the best hyperparameters with the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.650078, val_acc = 0.730000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.650074, val_acc = 0.730000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.603438, val_acc = 0.740000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.603436, val_acc = 0.740000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.649038, val_acc = 0.730000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.649039, val_acc = 0.730000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.597890, val_acc = 0.745000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.597890, val_acc = 0.745000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.543962, val_acc = 0.785000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.544000, val_acc = 0.785000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.526353, val_acc = 0.810000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.523589, val_acc = 0.810000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.520493, val_acc = 0.800000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.520492, val_acc = 0.800000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.456529, val_acc = 0.825000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.456530, val_acc = 0.825000\n",
      "\n",
      "The best parameters are: lr = 1e-05, alpha = 0.1, epoch = 300, tolerance = 0.0001\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "# choose best hyperparameters by using defined function \"choose_hyper()\" for BOW-count features\n",
    "best_parameter_bow_c = choose_hyper(train_count_mat, train_label, dev_count_mat, dev_label, \n",
    "                              lr_candidate = [1e-6, 1e-5], alpha_candidate = [5*1e-1, 1e-1], \n",
    "                              epoch_candidate = [100, 300], tolerance_candidate = [1e-4, 1e-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters tuning for BOW-COUNT model end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model for BOW-COUNT by using the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: training loss is 0.685818, validation loss is 0.687152.\n",
      "epoch   1: training loss is 0.679100, validation loss is 0.682006.\n",
      "epoch   2: training loss is 0.672717, validation loss is 0.677050.\n",
      "epoch   3: training loss is 0.666615, validation loss is 0.672550.\n",
      "epoch   4: training loss is 0.660792, validation loss is 0.668108.\n",
      "epoch   5: training loss is 0.655204, validation loss is 0.663948.\n",
      "epoch   6: training loss is 0.649844, validation loss is 0.659996.\n",
      "epoch   7: training loss is 0.644704, validation loss is 0.656128.\n",
      "epoch   8: training loss is 0.639744, validation loss is 0.652579.\n",
      "epoch   9: training loss is 0.634983, validation loss is 0.649160.\n",
      "epoch  10: training loss is 0.630359, validation loss is 0.645664.\n",
      "epoch  11: training loss is 0.625901, validation loss is 0.642309.\n",
      "epoch  12: training loss is 0.621592, validation loss is 0.639138.\n",
      "epoch  13: training loss is 0.617424, validation loss is 0.636024.\n",
      "epoch  14: training loss is 0.613396, validation loss is 0.633016.\n",
      "epoch  15: training loss is 0.609455, validation loss is 0.630225.\n",
      "epoch  16: training loss is 0.605641, validation loss is 0.627583.\n",
      "epoch  17: training loss is 0.601941, validation loss is 0.624960.\n",
      "epoch  18: training loss is 0.598337, validation loss is 0.622372.\n",
      "epoch  19: training loss is 0.594821, validation loss is 0.619823.\n",
      "epoch  20: training loss is 0.591398, validation loss is 0.617314.\n",
      "epoch  21: training loss is 0.588064, validation loss is 0.614834.\n",
      "epoch  22: training loss is 0.584811, validation loss is 0.612545.\n",
      "epoch  23: training loss is 0.581639, validation loss is 0.610226.\n",
      "epoch  24: training loss is 0.578537, validation loss is 0.608116.\n",
      "epoch  25: training loss is 0.575508, validation loss is 0.605948.\n",
      "epoch  26: training loss is 0.572546, validation loss is 0.603807.\n",
      "epoch  27: training loss is 0.569654, validation loss is 0.601859.\n",
      "epoch  28: training loss is 0.566828, validation loss is 0.599915.\n",
      "epoch  29: training loss is 0.564046, validation loss is 0.597834.\n",
      "epoch  30: training loss is 0.561330, validation loss is 0.595954.\n",
      "epoch  31: training loss is 0.558678, validation loss is 0.594188.\n",
      "epoch  32: training loss is 0.556065, validation loss is 0.592256.\n",
      "epoch  33: training loss is 0.553513, validation loss is 0.590512.\n",
      "epoch  34: training loss is 0.551008, validation loss is 0.588711.\n",
      "epoch  35: training loss is 0.548574, validation loss is 0.586851.\n",
      "epoch  36: training loss is 0.546151, validation loss is 0.585233.\n",
      "epoch  37: training loss is 0.543785, validation loss is 0.583617.\n",
      "epoch  38: training loss is 0.541457, validation loss is 0.582145.\n",
      "epoch  39: training loss is 0.539183, validation loss is 0.580439.\n",
      "epoch  40: training loss is 0.536958, validation loss is 0.578827.\n",
      "epoch  41: training loss is 0.534745, validation loss is 0.577376.\n",
      "epoch  42: training loss is 0.532574, validation loss is 0.575976.\n",
      "epoch  43: training loss is 0.530450, validation loss is 0.574600.\n",
      "epoch  44: training loss is 0.528359, validation loss is 0.573123.\n",
      "epoch  45: training loss is 0.526304, validation loss is 0.571745.\n",
      "epoch  46: training loss is 0.524285, validation loss is 0.570404.\n",
      "epoch  47: training loss is 0.522293, validation loss is 0.569019.\n",
      "epoch  48: training loss is 0.520341, validation loss is 0.567554.\n",
      "epoch  49: training loss is 0.518408, validation loss is 0.566329.\n",
      "epoch  50: training loss is 0.516511, validation loss is 0.565042.\n",
      "epoch  51: training loss is 0.514644, validation loss is 0.563807.\n",
      "epoch  52: training loss is 0.512805, validation loss is 0.562543.\n",
      "epoch  53: training loss is 0.510993, validation loss is 0.561354.\n",
      "epoch  54: training loss is 0.509214, validation loss is 0.560235.\n",
      "epoch  55: training loss is 0.507453, validation loss is 0.559018.\n",
      "epoch  56: training loss is 0.505725, validation loss is 0.557905.\n",
      "epoch  57: training loss is 0.504022, validation loss is 0.556793.\n",
      "epoch  58: training loss is 0.502327, validation loss is 0.555525.\n",
      "epoch  59: training loss is 0.500668, validation loss is 0.554427.\n",
      "epoch  60: training loss is 0.499031, validation loss is 0.553315.\n",
      "epoch  61: training loss is 0.497417, validation loss is 0.552259.\n",
      "epoch  62: training loss is 0.495825, validation loss is 0.551181.\n",
      "epoch  63: training loss is 0.494256, validation loss is 0.550092.\n",
      "epoch  64: training loss is 0.492711, validation loss is 0.549019.\n",
      "epoch  65: training loss is 0.491176, validation loss is 0.548108.\n",
      "epoch  66: training loss is 0.489667, validation loss is 0.547099.\n",
      "epoch  67: training loss is 0.488181, validation loss is 0.546042.\n",
      "epoch  68: training loss is 0.486706, validation loss is 0.545134.\n",
      "epoch  69: training loss is 0.485258, validation loss is 0.544271.\n",
      "epoch  70: training loss is 0.483824, validation loss is 0.543323.\n",
      "epoch  71: training loss is 0.482411, validation loss is 0.542417.\n",
      "epoch  72: training loss is 0.481007, validation loss is 0.541397.\n",
      "epoch  73: training loss is 0.479626, validation loss is 0.540467.\n",
      "epoch  74: training loss is 0.478261, validation loss is 0.539580.\n",
      "epoch  75: training loss is 0.476914, validation loss is 0.538662.\n",
      "epoch  76: training loss is 0.475583, validation loss is 0.537774.\n",
      "epoch  77: training loss is 0.474269, validation loss is 0.536899.\n",
      "epoch  78: training loss is 0.472969, validation loss is 0.536040.\n",
      "epoch  79: training loss is 0.471677, validation loss is 0.535290.\n",
      "epoch  80: training loss is 0.470407, validation loss is 0.534445.\n",
      "epoch  81: training loss is 0.469150, validation loss is 0.533639.\n",
      "epoch  82: training loss is 0.467913, validation loss is 0.532756.\n",
      "epoch  83: training loss is 0.466681, validation loss is 0.531986.\n",
      "epoch  84: training loss is 0.465467, validation loss is 0.531184.\n",
      "epoch  85: training loss is 0.464263, validation loss is 0.530435.\n",
      "epoch  86: training loss is 0.463074, validation loss is 0.529672.\n",
      "epoch  87: training loss is 0.461898, validation loss is 0.528916.\n",
      "epoch  88: training loss is 0.460734, validation loss is 0.528201.\n",
      "epoch  89: training loss is 0.459588, validation loss is 0.527388.\n",
      "epoch  90: training loss is 0.458449, validation loss is 0.526659.\n",
      "epoch  91: training loss is 0.457326, validation loss is 0.525908.\n",
      "epoch  92: training loss is 0.456206, validation loss is 0.525238.\n",
      "epoch  93: training loss is 0.455101, validation loss is 0.524596.\n",
      "epoch  94: training loss is 0.454009, validation loss is 0.523876.\n",
      "epoch  95: training loss is 0.452929, validation loss is 0.523160.\n",
      "epoch  96: training loss is 0.451858, validation loss is 0.522500.\n",
      "epoch  97: training loss is 0.450799, validation loss is 0.521838.\n",
      "epoch  98: training loss is 0.449753, validation loss is 0.521108.\n",
      "epoch  99: training loss is 0.448715, validation loss is 0.520440.\n",
      "epoch 100: training loss is 0.447686, validation loss is 0.519807.\n",
      "epoch 101: training loss is 0.446667, validation loss is 0.519169.\n",
      "epoch 102: training loss is 0.445659, validation loss is 0.518565.\n",
      "epoch 103: training loss is 0.444664, validation loss is 0.517980.\n",
      "epoch 104: training loss is 0.443679, validation loss is 0.517390.\n",
      "epoch 105: training loss is 0.442696, validation loss is 0.516737.\n",
      "epoch 106: training loss is 0.441722, validation loss is 0.516060.\n",
      "epoch 107: training loss is 0.440761, validation loss is 0.515433.\n",
      "epoch 108: training loss is 0.439809, validation loss is 0.514858.\n",
      "epoch 109: training loss is 0.438866, validation loss is 0.514212.\n",
      "epoch 110: training loss is 0.437932, validation loss is 0.513622.\n",
      "epoch 111: training loss is 0.437005, validation loss is 0.513085.\n",
      "epoch 112: training loss is 0.436087, validation loss is 0.512483.\n",
      "epoch 113: training loss is 0.435178, validation loss is 0.511897.\n",
      "epoch 114: training loss is 0.434276, validation loss is 0.511346.\n",
      "epoch 115: training loss is 0.433383, validation loss is 0.510832.\n",
      "epoch 116: training loss is 0.432509, validation loss is 0.510372.\n",
      "epoch 117: training loss is 0.431619, validation loss is 0.509705.\n",
      "epoch 118: training loss is 0.430749, validation loss is 0.509140.\n",
      "epoch 119: training loss is 0.429887, validation loss is 0.508589.\n",
      "epoch 120: training loss is 0.429031, validation loss is 0.508075.\n",
      "epoch 121: training loss is 0.428183, validation loss is 0.507551.\n",
      "epoch 122: training loss is 0.427342, validation loss is 0.507011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123: training loss is 0.426511, validation loss is 0.506548.\n",
      "epoch 124: training loss is 0.425683, validation loss is 0.506008.\n",
      "epoch 125: training loss is 0.424865, validation loss is 0.505521.\n",
      "epoch 126: training loss is 0.424051, validation loss is 0.505000.\n",
      "epoch 127: training loss is 0.423249, validation loss is 0.504543.\n",
      "epoch 128: training loss is 0.422446, validation loss is 0.504006.\n",
      "epoch 129: training loss is 0.421656, validation loss is 0.503544.\n",
      "epoch 130: training loss is 0.420867, validation loss is 0.503015.\n",
      "epoch 131: training loss is 0.420087, validation loss is 0.502545.\n",
      "epoch 132: training loss is 0.419313, validation loss is 0.502024.\n",
      "epoch 133: training loss is 0.418550, validation loss is 0.501496.\n",
      "epoch 134: training loss is 0.417785, validation loss is 0.501060.\n",
      "epoch 135: training loss is 0.417033, validation loss is 0.500564.\n",
      "epoch 136: training loss is 0.416280, validation loss is 0.500162.\n",
      "epoch 137: training loss is 0.415537, validation loss is 0.499681.\n",
      "epoch 138: training loss is 0.414798, validation loss is 0.499250.\n",
      "epoch 139: training loss is 0.414067, validation loss is 0.498775.\n",
      "epoch 140: training loss is 0.413340, validation loss is 0.498376.\n",
      "epoch 141: training loss is 0.412622, validation loss is 0.497983.\n",
      "epoch 142: training loss is 0.411906, validation loss is 0.497535.\n",
      "epoch 143: training loss is 0.411208, validation loss is 0.497195.\n",
      "epoch 144: training loss is 0.410490, validation loss is 0.496649.\n",
      "epoch 145: training loss is 0.409790, validation loss is 0.496204.\n",
      "epoch 146: training loss is 0.409097, validation loss is 0.495787.\n",
      "epoch 147: training loss is 0.408408, validation loss is 0.495370.\n",
      "epoch 148: training loss is 0.407724, validation loss is 0.494935.\n",
      "epoch 149: training loss is 0.407046, validation loss is 0.494501.\n",
      "epoch 150: training loss is 0.406372, validation loss is 0.494086.\n",
      "epoch 151: training loss is 0.405704, validation loss is 0.493670.\n",
      "epoch 152: training loss is 0.405040, validation loss is 0.493262.\n",
      "epoch 153: training loss is 0.404380, validation loss is 0.492889.\n",
      "epoch 154: training loss is 0.403726, validation loss is 0.492539.\n",
      "epoch 155: training loss is 0.403076, validation loss is 0.492140.\n",
      "epoch 156: training loss is 0.402431, validation loss is 0.491730.\n",
      "epoch 157: training loss is 0.401790, validation loss is 0.491338.\n",
      "epoch 158: training loss is 0.401154, validation loss is 0.490967.\n",
      "epoch 159: training loss is 0.400523, validation loss is 0.490557.\n",
      "epoch 160: training loss is 0.399896, validation loss is 0.490194.\n",
      "epoch 161: training loss is 0.399276, validation loss is 0.489770.\n",
      "epoch 162: training loss is 0.398656, validation loss is 0.489417.\n",
      "epoch 163: training loss is 0.398043, validation loss is 0.489046.\n",
      "epoch 164: training loss is 0.397432, validation loss is 0.488712.\n",
      "epoch 165: training loss is 0.396826, validation loss is 0.488343.\n",
      "epoch 166: training loss is 0.396225, validation loss is 0.487983.\n",
      "epoch 167: training loss is 0.395631, validation loss is 0.487702.\n",
      "epoch 168: training loss is 0.395037, validation loss is 0.487342.\n",
      "epoch 169: training loss is 0.394445, validation loss is 0.486938.\n",
      "epoch 170: training loss is 0.393860, validation loss is 0.486575.\n",
      "epoch 171: training loss is 0.393279, validation loss is 0.486248.\n",
      "epoch 172: training loss is 0.392701, validation loss is 0.485896.\n",
      "epoch 173: training loss is 0.392128, validation loss is 0.485569.\n",
      "epoch 174: training loss is 0.391558, validation loss is 0.485226.\n",
      "epoch 175: training loss is 0.390992, validation loss is 0.484866.\n",
      "epoch 176: training loss is 0.390430, validation loss is 0.484555.\n",
      "epoch 177: training loss is 0.389871, validation loss is 0.484232.\n",
      "epoch 178: training loss is 0.389316, validation loss is 0.483890.\n",
      "epoch 179: training loss is 0.388766, validation loss is 0.483602.\n",
      "epoch 180: training loss is 0.388216, validation loss is 0.483223.\n",
      "epoch 181: training loss is 0.387672, validation loss is 0.482898.\n",
      "epoch 182: training loss is 0.387132, validation loss is 0.482613.\n",
      "epoch 183: training loss is 0.386594, validation loss is 0.482295.\n",
      "epoch 184: training loss is 0.386060, validation loss is 0.481971.\n",
      "epoch 185: training loss is 0.385529, validation loss is 0.481657.\n",
      "epoch 186: training loss is 0.385002, validation loss is 0.481316.\n",
      "epoch 187: training loss is 0.384478, validation loss is 0.481039.\n",
      "epoch 188: training loss is 0.383957, validation loss is 0.480720.\n",
      "epoch 189: training loss is 0.383441, validation loss is 0.480456.\n",
      "epoch 190: training loss is 0.382927, validation loss is 0.480159.\n",
      "epoch 191: training loss is 0.382418, validation loss is 0.479878.\n",
      "epoch 192: training loss is 0.381908, validation loss is 0.479559.\n",
      "epoch 193: training loss is 0.381402, validation loss is 0.479229.\n",
      "epoch 194: training loss is 0.380900, validation loss is 0.478943.\n",
      "epoch 195: training loss is 0.380402, validation loss is 0.478618.\n",
      "epoch 196: training loss is 0.379906, validation loss is 0.478359.\n",
      "epoch 197: training loss is 0.379414, validation loss is 0.478083.\n",
      "epoch 198: training loss is 0.378925, validation loss is 0.477803.\n",
      "epoch 199: training loss is 0.378437, validation loss is 0.477486.\n",
      "epoch 200: training loss is 0.377954, validation loss is 0.477235.\n",
      "epoch 201: training loss is 0.377473, validation loss is 0.476914.\n",
      "epoch 202: training loss is 0.376995, validation loss is 0.476653.\n",
      "epoch 203: training loss is 0.376520, validation loss is 0.476387.\n",
      "epoch 204: training loss is 0.376048, validation loss is 0.476089.\n",
      "epoch 205: training loss is 0.375579, validation loss is 0.475825.\n",
      "epoch 206: training loss is 0.375112, validation loss is 0.475575.\n",
      "epoch 207: training loss is 0.374649, validation loss is 0.475325.\n",
      "epoch 208: training loss is 0.374187, validation loss is 0.475028.\n",
      "epoch 209: training loss is 0.373729, validation loss is 0.474750.\n",
      "epoch 210: training loss is 0.373273, validation loss is 0.474502.\n",
      "epoch 211: training loss is 0.372820, validation loss is 0.474215.\n",
      "epoch 212: training loss is 0.372369, validation loss is 0.473975.\n",
      "epoch 213: training loss is 0.371921, validation loss is 0.473709.\n",
      "epoch 214: training loss is 0.371476, validation loss is 0.473453.\n",
      "epoch 215: training loss is 0.371034, validation loss is 0.473189.\n",
      "epoch 216: training loss is 0.370593, validation loss is 0.472948.\n",
      "epoch 217: training loss is 0.370155, validation loss is 0.472711.\n",
      "epoch 218: training loss is 0.369719, validation loss is 0.472457.\n",
      "epoch 219: training loss is 0.369287, validation loss is 0.472190.\n",
      "epoch 220: training loss is 0.368860, validation loss is 0.471908.\n",
      "epoch 221: training loss is 0.368429, validation loss is 0.471685.\n",
      "epoch 222: training loss is 0.368003, validation loss is 0.471442.\n",
      "epoch 223: training loss is 0.367580, validation loss is 0.471197.\n",
      "epoch 224: training loss is 0.367160, validation loss is 0.470947.\n",
      "epoch 225: training loss is 0.366740, validation loss is 0.470728.\n",
      "epoch 226: training loss is 0.366323, validation loss is 0.470494.\n",
      "epoch 227: training loss is 0.365910, validation loss is 0.470247.\n",
      "epoch 228: training loss is 0.365498, validation loss is 0.470016.\n",
      "epoch 229: training loss is 0.365088, validation loss is 0.469802.\n",
      "epoch 230: training loss is 0.364680, validation loss is 0.469577.\n",
      "epoch 231: training loss is 0.364275, validation loss is 0.469351.\n",
      "epoch 232: training loss is 0.363874, validation loss is 0.469156.\n",
      "epoch 233: training loss is 0.363474, validation loss is 0.468940.\n",
      "epoch 234: training loss is 0.363075, validation loss is 0.468709.\n",
      "epoch 235: training loss is 0.362682, validation loss is 0.468517.\n",
      "epoch 236: training loss is 0.362284, validation loss is 0.468258.\n",
      "epoch 237: training loss is 0.361893, validation loss is 0.468038.\n",
      "epoch 238: training loss is 0.361502, validation loss is 0.467758.\n",
      "epoch 239: training loss is 0.361114, validation loss is 0.467554.\n",
      "epoch 240: training loss is 0.360729, validation loss is 0.467384.\n",
      "epoch 241: training loss is 0.360344, validation loss is 0.467142.\n",
      "epoch 242: training loss is 0.359962, validation loss is 0.466928.\n",
      "epoch 243: training loss is 0.359583, validation loss is 0.466720.\n",
      "epoch 244: training loss is 0.359205, validation loss is 0.466512.\n",
      "epoch 245: training loss is 0.358832, validation loss is 0.466327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 246: training loss is 0.358456, validation loss is 0.466087.\n",
      "epoch 247: training loss is 0.358084, validation loss is 0.465890.\n",
      "epoch 248: training loss is 0.357715, validation loss is 0.465693.\n",
      "epoch 249: training loss is 0.357345, validation loss is 0.465452.\n",
      "epoch 250: training loss is 0.356981, validation loss is 0.465285.\n",
      "epoch 251: training loss is 0.356615, validation loss is 0.465045.\n",
      "epoch 252: training loss is 0.356253, validation loss is 0.464824.\n",
      "epoch 253: training loss is 0.355892, validation loss is 0.464620.\n",
      "epoch 254: training loss is 0.355534, validation loss is 0.464411.\n",
      "epoch 255: training loss is 0.355177, validation loss is 0.464225.\n",
      "epoch 256: training loss is 0.354822, validation loss is 0.464047.\n",
      "epoch 257: training loss is 0.354470, validation loss is 0.463878.\n",
      "epoch 258: training loss is 0.354119, validation loss is 0.463681.\n",
      "epoch 259: training loss is 0.353768, validation loss is 0.463457.\n",
      "epoch 260: training loss is 0.353420, validation loss is 0.463258.\n",
      "epoch 261: training loss is 0.353074, validation loss is 0.463062.\n",
      "epoch 262: training loss is 0.352731, validation loss is 0.462846.\n",
      "epoch 263: training loss is 0.352389, validation loss is 0.462652.\n",
      "epoch 264: training loss is 0.352047, validation loss is 0.462473.\n",
      "epoch 265: training loss is 0.351707, validation loss is 0.462294.\n",
      "epoch 266: training loss is 0.351369, validation loss is 0.462118.\n",
      "epoch 267: training loss is 0.351033, validation loss is 0.461930.\n",
      "epoch 268: training loss is 0.350700, validation loss is 0.461774.\n",
      "epoch 269: training loss is 0.350368, validation loss is 0.461604.\n",
      "epoch 270: training loss is 0.350036, validation loss is 0.461411.\n",
      "epoch 271: training loss is 0.349707, validation loss is 0.461231.\n",
      "epoch 272: training loss is 0.349382, validation loss is 0.461075.\n",
      "epoch 273: training loss is 0.349053, validation loss is 0.460867.\n",
      "epoch 274: training loss is 0.348730, validation loss is 0.460704.\n",
      "epoch 275: training loss is 0.348406, validation loss is 0.460479.\n",
      "epoch 276: training loss is 0.348085, validation loss is 0.460295.\n",
      "epoch 277: training loss is 0.347765, validation loss is 0.460141.\n",
      "epoch 278: training loss is 0.347446, validation loss is 0.459968.\n",
      "epoch 279: training loss is 0.347130, validation loss is 0.459787.\n",
      "epoch 280: training loss is 0.346816, validation loss is 0.459596.\n",
      "epoch 281: training loss is 0.346502, validation loss is 0.459425.\n",
      "epoch 282: training loss is 0.346190, validation loss is 0.459251.\n",
      "epoch 283: training loss is 0.345879, validation loss is 0.459084.\n",
      "epoch 284: training loss is 0.345570, validation loss is 0.458910.\n",
      "epoch 285: training loss is 0.345261, validation loss is 0.458757.\n",
      "epoch 286: training loss is 0.344955, validation loss is 0.458590.\n",
      "epoch 287: training loss is 0.344650, validation loss is 0.458452.\n",
      "epoch 288: training loss is 0.344347, validation loss is 0.458289.\n",
      "epoch 289: training loss is 0.344045, validation loss is 0.458098.\n",
      "epoch 290: training loss is 0.343745, validation loss is 0.457962.\n",
      "epoch 291: training loss is 0.343445, validation loss is 0.457795.\n",
      "epoch 292: training loss is 0.343148, validation loss is 0.457600.\n",
      "epoch 293: training loss is 0.342851, validation loss is 0.457457.\n",
      "epoch 294: training loss is 0.342557, validation loss is 0.457281.\n",
      "epoch 295: training loss is 0.342264, validation loss is 0.457120.\n",
      "epoch 296: training loss is 0.341971, validation loss is 0.456987.\n",
      "epoch 297: training loss is 0.341680, validation loss is 0.456840.\n",
      "epoch 298: training loss is 0.341392, validation loss is 0.456649.\n",
      "epoch 299: training loss is 0.341103, validation loss is 0.456499.\n"
     ]
    }
   ],
   "source": [
    "# after choosing the best parameters, we could use these parameters to train the model for BOW-count features.\n",
    "\n",
    "weights_bow_c, tra_loss_bow_c, val_loss_bow_c = SGD(train_count_mat, train_label, dev_count_mat, dev_label,\n",
    "                                                    lr = best_parameter_bow_c[0], \n",
    "                                                    alpha = best_parameter_bow_c[1],\n",
    "                                                    epochs = best_parameter_bow_c[2],\n",
    "                                                    tolerance = best_parameter_bow_c[3], print_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now plot the training and validation history per epoch for the best hyperparameter combination. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.598911Z",
     "start_time": "2020-02-15T14:17:51.482307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a function to plot the training and validation history \n",
    "\n",
    "def plot_loss_history(tra_loss_history, val_loss_history, mode):\n",
    "    \n",
    "    x_value = np.array([i for i in range(len(tra_loss_history))])\n",
    "    \n",
    "    plt.plot(x_value,tra_loss_history, lw = 2, label = \"training loss\")\n",
    "    plt.plot(x_value, val_loss_history, lw = 2, label = \"validation loss\")\n",
    "    \n",
    "    \n",
    "    plt.title(mode)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.ylim([0, 0.8])\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plot_loss_history(tra_loss_bow_c, val_loss_bow_c, \"BOW-COUNT(fig-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "Explain here...\n",
    "\n",
    "Because we have five models, we will analyze this question after the \"loss-epoch\" figure for each model.\n",
    "\n",
    "From the figure 1, we could find our mode is trained well. \n",
    "\n",
    "1. The validation loss line is small (about 0.45) and is becoming flat at the end of training. Besides, the test accuracy of the following evaluation part is high (about 83.25%). Therefore the model is not underfitted.\n",
    "2. Because the validation loss line doesn't tend to rise and is still in the process from declining to flatting, and the difference between the final training loss and the final validation loss is small enough (about 0.12), the model is not overfitted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.607940Z",
     "start_time": "2020-02-15T14:17:51.600272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8225\n",
      "Precision: 0.8056872037914692\n",
      "Recall: 0.85\n",
      "F1-Score: 0.8272506082725061\n"
     ]
    }
   ],
   "source": [
    "preds_bow_c = predict_class(test_count_mat, weights_bow_c)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_label, preds_bow_c))\n",
    "print('Precision:', precision_score(test_label, preds_bow_c))\n",
    "print('Recall:', recall_score(test_label, preds_bow_c))\n",
    "print('F1-Score:', f1_score(test_label, preds_bow_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.613935Z",
     "start_time": "2020-02-15T14:17:51.610660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "only\n",
      "plot\n",
      "even\n",
      "no\n",
      "any\n",
      "worst\n",
      "script\n",
      "nothing\n",
      "unfortunately\n"
     ]
    }
   ],
   "source": [
    "top_neg = weights_bow_c.reshape(1, -1).argsort()[0][:10]\n",
    "for i in top_neg:\n",
    "    print(id2word_bow[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.624122Z",
     "start_time": "2020-02-15T14:17:51.615674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "well\n",
      "life\n",
      "world\n",
      "many\n",
      "best\n",
      "seen\n",
      "both\n",
      "very\n",
      "most\n"
     ]
    }
   ],
   "source": [
    "top_pos = weights_bow_c.reshape(1, -1).argsort()[0][::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id2word_bow[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your answer here...\n",
    "\n",
    "Because we have five models, we will analyze this question for each model after its own top-10 words/chars.\n",
    "\n",
    "From the observation on these return words, I think half of them make sense. For example, in the top-10 negative words, we could see the negative words like \"bad\", \"worst\", \"nothing\", \"unfortunately\" and, in the top-10 positive words, we could see the positive words like \"great\", \"well\", \"best\", \"very\".\n",
    "\n",
    "I think these aforementioned words, such as \"bad\", \"great\", \"best\", \"well\", and, \"worst\", could generalize well in other domains like a restaurant reviews or laptop reviews. Because these words could express the opinions of customers to any kinds of merchandises and services. However, there are also some words like \"plot\" and \"script\" not generalizing well. Because these two words are only common in movies' area.\n",
    "\n",
    "The classifier should use the following words to express the emotions of customers or to express the opinions of customers to the merchandises for generalization. For example, the words \"happy\", \"surprised\", \"sad\", and \"disappointed\" could be used to express the emotions of customers in every domain and the words \"great\", \"worthy\", \"expensive\", and \"bad\" could be used in every domain because they could express the opinions of customers to most kind of merchandises or services.\n",
    "\n",
    "For the restaurant reviews domain, the classifier may pick up words like \"delicious\", \"tasty\", \"smelly\", \"expensive\", \"cold\", \"fresh\".... and for laptop review domain, the classifier may pick up words like \"light\", \"beautiful\", \"fast\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your answer here...\n",
    "\n",
    "We have five models, so I will answer this question for each model.\n",
    "\n",
    "1. I choose the hyperparameters (learning rate, alpha, epoch, tolerance) by testing the development(validation) dataset on several different set of hyperparameters. The set of hyperparameters which could help the model to achieve the highest validation accuracy and lowest validation loss are the best hyperparameters. I will randomly display several set of hyperparameters and the corresponding validation loss and validation accuracy.\n",
    "\n",
    "| lr | alpha |epoch  | tolerance | val_loss | val_accuracy |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 0.000010  | 0.100000 | 300  |  0.000100 | 0.456529  | 0.825000  |\n",
    "| 0.000010  |  0.500000| 300  |  0.000100  | 0.526353 | 0.810000  |\n",
    "| 0.000010  | 0.100000   |100  | 0.000100 | 0.520493  | 0.800000  |\n",
    "| 0.000001 | 0.100000  |  300  | 0.000100  | 0.597890  | 0.745000  |\n",
    "| 0.000010  | 0.100000   |  300 | 0.000010  |0.456530  | 0.825000 |\n",
    "\n",
    "From the above table, we could find when the lr = 1e-5, alpha = 0.1, epoch = 300, tolerance = 1e-4, the model has lowest validation loss (0.456) and highest validation accuracy (0.825). Therefore, I choose this set of hyperparameter as my best hyperparameter for model of BOW-COUNT.\n",
    "\n",
    "2. From the above table and training process, I find if the learning rate is small, then the model need lost of epochs to complete the training to achieve a high validation accuracy. If the learning rate is large, the model will only need a few epoch to complete the training.\n",
    "\n",
    "\n",
    "3. From the above table, we could know the regularization strength could help the model not be overtrained. It means, if we use a large regularization strength (alpha), then the training loss will decrease more slowly than using a small regularization strength. If we don't add a regularization term or use a small regularization strength, the model might be overfitted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now repeat the training and evaluation process for BOW-tfidf, BOCN-count, BOCN-tfidf, BOW+BOCN including hyperparameter tuning process for each model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: BOW-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation for BOW-tfidf\n",
    "\n",
    "Because we have get the vectorized features matrices for BOW-tfidf, we could us them directly.\n",
    "\n",
    "Please don't count the hyperparameters tunning time for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.669359, val_acc = 0.695000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.669358, val_acc = 0.695000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.636845, val_acc = 0.760000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.636845, val_acc = 0.760000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.668735, val_acc = 0.700000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.668735, val_acc = 0.700000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.632733, val_acc = 0.760000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.632733, val_acc = 0.760000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.583639, val_acc = 0.780000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.583637, val_acc = 0.780000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.557859, val_acc = 0.795000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.552537, val_acc = 0.800000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.561617, val_acc = 0.800000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.561618, val_acc = 0.800000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.490416, val_acc = 0.835000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.490415, val_acc = 0.835000\n",
      "\n",
      "The best parameters are: lr = 1e-05, alpha = 0.1, epoch = 300, tolerance = 1e-05\n"
     ]
    }
   ],
   "source": [
    "# choose best hyperparameters for BOW-tfidf\n",
    "\n",
    "best_parameter_bow_tfidf = choose_hyper(train_tfidf_mat, train_label, dev_tfidf_mat, dev_label, \n",
    "                              lr_candidate = [1e-6, 1e-5], alpha_candidate = [5*1e-1, 1e-1], \n",
    "                              epoch_candidate = [100, 300], tolerance_candidate = [1e-4, 1e-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of hyperparameters tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model of BOW-TFIDF by using the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: training loss is 0.688411, validation loss is 0.690324.\n",
      "epoch   1: training loss is 0.683823, validation loss is 0.687615.\n",
      "epoch   2: training loss is 0.679373, validation loss is 0.685008.\n",
      "epoch   3: training loss is 0.675039, validation loss is 0.682484.\n",
      "epoch   4: training loss is 0.670819, validation loss is 0.680043.\n",
      "epoch   5: training loss is 0.666696, validation loss is 0.677668.\n",
      "epoch   6: training loss is 0.662661, validation loss is 0.675347.\n",
      "epoch   7: training loss is 0.658714, validation loss is 0.673084.\n",
      "epoch   8: training loss is 0.654847, validation loss is 0.670872.\n",
      "epoch   9: training loss is 0.651058, validation loss is 0.668713.\n",
      "epoch  10: training loss is 0.647342, validation loss is 0.666596.\n",
      "epoch  11: training loss is 0.643694, validation loss is 0.664522.\n",
      "epoch  12: training loss is 0.640115, validation loss is 0.662493.\n",
      "epoch  13: training loss is 0.636598, validation loss is 0.660495.\n",
      "epoch  14: training loss is 0.633143, validation loss is 0.658531.\n",
      "epoch  15: training loss is 0.629749, validation loss is 0.656608.\n",
      "epoch  16: training loss is 0.626412, validation loss is 0.654712.\n",
      "epoch  17: training loss is 0.623131, validation loss is 0.652850.\n",
      "epoch  18: training loss is 0.619905, validation loss is 0.651016.\n",
      "epoch  19: training loss is 0.616733, validation loss is 0.649215.\n",
      "epoch  20: training loss is 0.613612, validation loss is 0.647443.\n",
      "epoch  21: training loss is 0.610542, validation loss is 0.645700.\n",
      "epoch  22: training loss is 0.607522, validation loss is 0.643989.\n",
      "epoch  23: training loss is 0.604548, validation loss is 0.642297.\n",
      "epoch  24: training loss is 0.601621, validation loss is 0.640636.\n",
      "epoch  25: training loss is 0.598739, validation loss is 0.638999.\n",
      "epoch  26: training loss is 0.595902, validation loss is 0.637389.\n",
      "epoch  27: training loss is 0.593108, validation loss is 0.635800.\n",
      "epoch  28: training loss is 0.590355, validation loss is 0.634232.\n",
      "epoch  29: training loss is 0.587644, validation loss is 0.632691.\n",
      "epoch  30: training loss is 0.584974, validation loss is 0.631173.\n",
      "epoch  31: training loss is 0.582342, validation loss is 0.629680.\n",
      "epoch  32: training loss is 0.579749, validation loss is 0.628206.\n",
      "epoch  33: training loss is 0.577192, validation loss is 0.626749.\n",
      "epoch  34: training loss is 0.574673, validation loss is 0.625312.\n",
      "epoch  35: training loss is 0.572189, validation loss is 0.623898.\n",
      "epoch  36: training loss is 0.569740, validation loss is 0.622505.\n",
      "epoch  37: training loss is 0.567325, validation loss is 0.621136.\n",
      "epoch  38: training loss is 0.564943, validation loss is 0.619777.\n",
      "epoch  39: training loss is 0.562594, validation loss is 0.618443.\n",
      "epoch  40: training loss is 0.560276, validation loss is 0.617123.\n",
      "epoch  41: training loss is 0.557990, validation loss is 0.615823.\n",
      "epoch  42: training loss is 0.555735, validation loss is 0.614543.\n",
      "epoch  43: training loss is 0.553509, validation loss is 0.613276.\n",
      "epoch  44: training loss is 0.551312, validation loss is 0.612025.\n",
      "epoch  45: training loss is 0.549144, validation loss is 0.610787.\n",
      "epoch  46: training loss is 0.547004, validation loss is 0.609569.\n",
      "epoch  47: training loss is 0.544891, validation loss is 0.608364.\n",
      "epoch  48: training loss is 0.542805, validation loss is 0.607177.\n",
      "epoch  49: training loss is 0.540745, validation loss is 0.606005.\n",
      "epoch  50: training loss is 0.538711, validation loss is 0.604846.\n",
      "epoch  51: training loss is 0.536702, validation loss is 0.603707.\n",
      "epoch  52: training loss is 0.534718, validation loss is 0.602577.\n",
      "epoch  53: training loss is 0.532758, validation loss is 0.601462.\n",
      "epoch  54: training loss is 0.530822, validation loss is 0.600359.\n",
      "epoch  55: training loss is 0.528909, validation loss is 0.599270.\n",
      "epoch  56: training loss is 0.527019, validation loss is 0.598194.\n",
      "epoch  57: training loss is 0.525151, validation loss is 0.597132.\n",
      "epoch  58: training loss is 0.523306, validation loss is 0.596080.\n",
      "epoch  59: training loss is 0.521482, validation loss is 0.595044.\n",
      "epoch  60: training loss is 0.519679, validation loss is 0.594020.\n",
      "epoch  61: training loss is 0.517897, validation loss is 0.593009.\n",
      "epoch  62: training loss is 0.516135, validation loss is 0.592008.\n",
      "epoch  63: training loss is 0.514393, validation loss is 0.591017.\n",
      "epoch  64: training loss is 0.512671, validation loss is 0.590037.\n",
      "epoch  65: training loss is 0.510968, validation loss is 0.589070.\n",
      "epoch  66: training loss is 0.509283, validation loss is 0.588112.\n",
      "epoch  67: training loss is 0.507618, validation loss is 0.587168.\n",
      "epoch  68: training loss is 0.505971, validation loss is 0.586234.\n",
      "epoch  69: training loss is 0.504342, validation loss is 0.585309.\n",
      "epoch  70: training loss is 0.502730, validation loss is 0.584396.\n",
      "epoch  71: training loss is 0.501136, validation loss is 0.583487.\n",
      "epoch  72: training loss is 0.499558, validation loss is 0.582590.\n",
      "epoch  73: training loss is 0.497998, validation loss is 0.581706.\n",
      "epoch  74: training loss is 0.496453, validation loss is 0.580832.\n",
      "epoch  75: training loss is 0.494925, validation loss is 0.579967.\n",
      "epoch  76: training loss is 0.493413, validation loss is 0.579110.\n",
      "epoch  77: training loss is 0.491916, validation loss is 0.578265.\n",
      "epoch  78: training loss is 0.490435, validation loss is 0.577426.\n",
      "epoch  79: training loss is 0.488969, validation loss is 0.576596.\n",
      "epoch  80: training loss is 0.487517, validation loss is 0.575774.\n",
      "epoch  81: training loss is 0.486081, validation loss is 0.574960.\n",
      "epoch  82: training loss is 0.484658, validation loss is 0.574155.\n",
      "epoch  83: training loss is 0.483250, validation loss is 0.573358.\n",
      "epoch  84: training loss is 0.481856, validation loss is 0.572569.\n",
      "epoch  85: training loss is 0.480475, validation loss is 0.571788.\n",
      "epoch  86: training loss is 0.479108, validation loss is 0.571013.\n",
      "epoch  87: training loss is 0.477754, validation loss is 0.570249.\n",
      "epoch  88: training loss is 0.476414, validation loss is 0.569490.\n",
      "epoch  89: training loss is 0.475086, validation loss is 0.568740.\n",
      "epoch  90: training loss is 0.473771, validation loss is 0.567997.\n",
      "epoch  91: training loss is 0.472468, validation loss is 0.567261.\n",
      "epoch  92: training loss is 0.471177, validation loss is 0.566531.\n",
      "epoch  93: training loss is 0.469899, validation loss is 0.565811.\n",
      "epoch  94: training loss is 0.468633, validation loss is 0.565097.\n",
      "epoch  95: training loss is 0.467378, validation loss is 0.564388.\n",
      "epoch  96: training loss is 0.466134, validation loss is 0.563685.\n",
      "epoch  97: training loss is 0.464903, validation loss is 0.562990.\n",
      "epoch  98: training loss is 0.463682, validation loss is 0.562301.\n",
      "epoch  99: training loss is 0.462472, validation loss is 0.561620.\n",
      "epoch 100: training loss is 0.461274, validation loss is 0.560944.\n",
      "epoch 101: training loss is 0.460086, validation loss is 0.560275.\n",
      "epoch 102: training loss is 0.458909, validation loss is 0.559611.\n",
      "epoch 103: training loss is 0.457742, validation loss is 0.558953.\n",
      "epoch 104: training loss is 0.456585, validation loss is 0.558302.\n",
      "epoch 105: training loss is 0.455439, validation loss is 0.557656.\n",
      "epoch 106: training loss is 0.454302, validation loss is 0.557016.\n",
      "epoch 107: training loss is 0.453176, validation loss is 0.556381.\n",
      "epoch 108: training loss is 0.452059, validation loss is 0.555753.\n",
      "epoch 109: training loss is 0.450952, validation loss is 0.555129.\n",
      "epoch 110: training loss is 0.449854, validation loss is 0.554512.\n",
      "epoch 111: training loss is 0.448765, validation loss is 0.553900.\n",
      "epoch 112: training loss is 0.447686, validation loss is 0.553294.\n",
      "epoch 113: training loss is 0.446616, validation loss is 0.552691.\n",
      "epoch 114: training loss is 0.445555, validation loss is 0.552095.\n",
      "epoch 115: training loss is 0.444503, validation loss is 0.551504.\n",
      "epoch 116: training loss is 0.443459, validation loss is 0.550917.\n",
      "epoch 117: training loss is 0.442424, validation loss is 0.550336.\n",
      "epoch 118: training loss is 0.441398, validation loss is 0.549759.\n",
      "epoch 119: training loss is 0.440380, validation loss is 0.549188.\n",
      "epoch 120: training loss is 0.439370, validation loss is 0.548620.\n",
      "epoch 121: training loss is 0.438368, validation loss is 0.548057.\n",
      "epoch 122: training loss is 0.437375, validation loss is 0.547500.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123: training loss is 0.436389, validation loss is 0.546946.\n",
      "epoch 124: training loss is 0.435411, validation loss is 0.546397.\n",
      "epoch 125: training loss is 0.434441, validation loss is 0.545854.\n",
      "epoch 126: training loss is 0.433479, validation loss is 0.545314.\n",
      "epoch 127: training loss is 0.432525, validation loss is 0.544779.\n",
      "epoch 128: training loss is 0.431578, validation loss is 0.544248.\n",
      "epoch 129: training loss is 0.430638, validation loss is 0.543721.\n",
      "epoch 130: training loss is 0.429706, validation loss is 0.543200.\n",
      "epoch 131: training loss is 0.428780, validation loss is 0.542682.\n",
      "epoch 132: training loss is 0.427862, validation loss is 0.542167.\n",
      "epoch 133: training loss is 0.426951, validation loss is 0.541657.\n",
      "epoch 134: training loss is 0.426047, validation loss is 0.541151.\n",
      "epoch 135: training loss is 0.425150, validation loss is 0.540649.\n",
      "epoch 136: training loss is 0.424259, validation loss is 0.540149.\n",
      "epoch 137: training loss is 0.423376, validation loss is 0.539655.\n",
      "epoch 138: training loss is 0.422499, validation loss is 0.539164.\n",
      "epoch 139: training loss is 0.421628, validation loss is 0.538678.\n",
      "epoch 140: training loss is 0.420764, validation loss is 0.538194.\n",
      "epoch 141: training loss is 0.419907, validation loss is 0.537715.\n",
      "epoch 142: training loss is 0.419055, validation loss is 0.537239.\n",
      "epoch 143: training loss is 0.418210, validation loss is 0.536767.\n",
      "epoch 144: training loss is 0.417371, validation loss is 0.536297.\n",
      "epoch 145: training loss is 0.416539, validation loss is 0.535832.\n",
      "epoch 146: training loss is 0.415712, validation loss is 0.535370.\n",
      "epoch 147: training loss is 0.414891, validation loss is 0.534912.\n",
      "epoch 148: training loss is 0.414077, validation loss is 0.534457.\n",
      "epoch 149: training loss is 0.413268, validation loss is 0.534006.\n",
      "epoch 150: training loss is 0.412465, validation loss is 0.533558.\n",
      "epoch 151: training loss is 0.411667, validation loss is 0.533112.\n",
      "epoch 152: training loss is 0.410875, validation loss is 0.532670.\n",
      "epoch 153: training loss is 0.410089, validation loss is 0.532232.\n",
      "epoch 154: training loss is 0.409309, validation loss is 0.531797.\n",
      "epoch 155: training loss is 0.408533, validation loss is 0.531365.\n",
      "epoch 156: training loss is 0.407764, validation loss is 0.530935.\n",
      "epoch 157: training loss is 0.406999, validation loss is 0.530508.\n",
      "epoch 158: training loss is 0.406240, validation loss is 0.530085.\n",
      "epoch 159: training loss is 0.405486, validation loss is 0.529665.\n",
      "epoch 160: training loss is 0.404738, validation loss is 0.529248.\n",
      "epoch 161: training loss is 0.403994, validation loss is 0.528834.\n",
      "epoch 162: training loss is 0.403256, validation loss is 0.528422.\n",
      "epoch 163: training loss is 0.402522, validation loss is 0.528013.\n",
      "epoch 164: training loss is 0.401794, validation loss is 0.527607.\n",
      "epoch 165: training loss is 0.401070, validation loss is 0.527204.\n",
      "epoch 166: training loss is 0.400352, validation loss is 0.526803.\n",
      "epoch 167: training loss is 0.399638, validation loss is 0.526406.\n",
      "epoch 168: training loss is 0.398929, validation loss is 0.526011.\n",
      "epoch 169: training loss is 0.398224, validation loss is 0.525619.\n",
      "epoch 170: training loss is 0.397525, validation loss is 0.525229.\n",
      "epoch 171: training loss is 0.396830, validation loss is 0.524843.\n",
      "epoch 172: training loss is 0.396139, validation loss is 0.524459.\n",
      "epoch 173: training loss is 0.395453, validation loss is 0.524077.\n",
      "epoch 174: training loss is 0.394772, validation loss is 0.523698.\n",
      "epoch 175: training loss is 0.394095, validation loss is 0.523322.\n",
      "epoch 176: training loss is 0.393422, validation loss is 0.522948.\n",
      "epoch 177: training loss is 0.392754, validation loss is 0.522577.\n",
      "epoch 178: training loss is 0.392090, validation loss is 0.522208.\n",
      "epoch 179: training loss is 0.391430, validation loss is 0.521841.\n",
      "epoch 180: training loss is 0.390775, validation loss is 0.521477.\n",
      "epoch 181: training loss is 0.390124, validation loss is 0.521115.\n",
      "epoch 182: training loss is 0.389477, validation loss is 0.520755.\n",
      "epoch 183: training loss is 0.388834, validation loss is 0.520398.\n",
      "epoch 184: training loss is 0.388195, validation loss is 0.520043.\n",
      "epoch 185: training loss is 0.387560, validation loss is 0.519691.\n",
      "epoch 186: training loss is 0.386929, validation loss is 0.519340.\n",
      "epoch 187: training loss is 0.386302, validation loss is 0.518992.\n",
      "epoch 188: training loss is 0.385679, validation loss is 0.518647.\n",
      "epoch 189: training loss is 0.385060, validation loss is 0.518302.\n",
      "epoch 190: training loss is 0.384445, validation loss is 0.517961.\n",
      "epoch 191: training loss is 0.383833, validation loss is 0.517621.\n",
      "epoch 192: training loss is 0.383225, validation loss is 0.517284.\n",
      "epoch 193: training loss is 0.382621, validation loss is 0.516949.\n",
      "epoch 194: training loss is 0.382021, validation loss is 0.516616.\n",
      "epoch 195: training loss is 0.381425, validation loss is 0.516285.\n",
      "epoch 196: training loss is 0.380832, validation loss is 0.515956.\n",
      "epoch 197: training loss is 0.380242, validation loss is 0.515629.\n",
      "epoch 198: training loss is 0.379656, validation loss is 0.515304.\n",
      "epoch 199: training loss is 0.379074, validation loss is 0.514982.\n",
      "epoch 200: training loss is 0.378496, validation loss is 0.514660.\n",
      "epoch 201: training loss is 0.377920, validation loss is 0.514342.\n",
      "epoch 202: training loss is 0.377348, validation loss is 0.514025.\n",
      "epoch 203: training loss is 0.376780, validation loss is 0.513710.\n",
      "epoch 204: training loss is 0.376215, validation loss is 0.513397.\n",
      "epoch 205: training loss is 0.375653, validation loss is 0.513086.\n",
      "epoch 206: training loss is 0.375095, validation loss is 0.512777.\n",
      "epoch 207: training loss is 0.374540, validation loss is 0.512469.\n",
      "epoch 208: training loss is 0.373988, validation loss is 0.512163.\n",
      "epoch 209: training loss is 0.373440, validation loss is 0.511860.\n",
      "epoch 210: training loss is 0.372894, validation loss is 0.511558.\n",
      "epoch 211: training loss is 0.372352, validation loss is 0.511258.\n",
      "epoch 212: training loss is 0.371813, validation loss is 0.510959.\n",
      "epoch 213: training loss is 0.371277, validation loss is 0.510662.\n",
      "epoch 214: training loss is 0.370744, validation loss is 0.510367.\n",
      "epoch 215: training loss is 0.370215, validation loss is 0.510074.\n",
      "epoch 216: training loss is 0.369688, validation loss is 0.509783.\n",
      "epoch 217: training loss is 0.369164, validation loss is 0.509493.\n",
      "epoch 218: training loss is 0.368644, validation loss is 0.509205.\n",
      "epoch 219: training loss is 0.368126, validation loss is 0.508919.\n",
      "epoch 220: training loss is 0.367611, validation loss is 0.508635.\n",
      "epoch 221: training loss is 0.367099, validation loss is 0.508352.\n",
      "epoch 222: training loss is 0.366590, validation loss is 0.508070.\n",
      "epoch 223: training loss is 0.366084, validation loss is 0.507790.\n",
      "epoch 224: training loss is 0.365581, validation loss is 0.507512.\n",
      "epoch 225: training loss is 0.365081, validation loss is 0.507235.\n",
      "epoch 226: training loss is 0.364583, validation loss is 0.506960.\n",
      "epoch 227: training loss is 0.364088, validation loss is 0.506687.\n",
      "epoch 228: training loss is 0.363596, validation loss is 0.506415.\n",
      "epoch 229: training loss is 0.363107, validation loss is 0.506144.\n",
      "epoch 230: training loss is 0.362620, validation loss is 0.505875.\n",
      "epoch 231: training loss is 0.362137, validation loss is 0.505608.\n",
      "epoch 232: training loss is 0.361655, validation loss is 0.505342.\n",
      "epoch 233: training loss is 0.361177, validation loss is 0.505078.\n",
      "epoch 234: training loss is 0.360701, validation loss is 0.504815.\n",
      "epoch 235: training loss is 0.360227, validation loss is 0.504554.\n",
      "epoch 236: training loss is 0.359757, validation loss is 0.504294.\n",
      "epoch 237: training loss is 0.359288, validation loss is 0.504035.\n",
      "epoch 238: training loss is 0.358823, validation loss is 0.503778.\n",
      "epoch 239: training loss is 0.358360, validation loss is 0.503522.\n",
      "epoch 240: training loss is 0.357899, validation loss is 0.503268.\n",
      "epoch 241: training loss is 0.357441, validation loss is 0.503016.\n",
      "epoch 242: training loss is 0.356985, validation loss is 0.502764.\n",
      "epoch 243: training loss is 0.356532, validation loss is 0.502514.\n",
      "epoch 244: training loss is 0.356081, validation loss is 0.502266.\n",
      "epoch 245: training loss is 0.355632, validation loss is 0.502018.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 246: training loss is 0.355186, validation loss is 0.501772.\n",
      "epoch 247: training loss is 0.354743, validation loss is 0.501528.\n",
      "epoch 248: training loss is 0.354301, validation loss is 0.501284.\n",
      "epoch 249: training loss is 0.353862, validation loss is 0.501042.\n",
      "epoch 250: training loss is 0.353426, validation loss is 0.500802.\n",
      "epoch 251: training loss is 0.352991, validation loss is 0.500562.\n",
      "epoch 252: training loss is 0.352559, validation loss is 0.500324.\n",
      "epoch 253: training loss is 0.352129, validation loss is 0.500087.\n",
      "epoch 254: training loss is 0.351702, validation loss is 0.499851.\n",
      "epoch 255: training loss is 0.351277, validation loss is 0.499617.\n",
      "epoch 256: training loss is 0.350853, validation loss is 0.499384.\n",
      "epoch 257: training loss is 0.350432, validation loss is 0.499152.\n",
      "epoch 258: training loss is 0.350014, validation loss is 0.498921.\n",
      "epoch 259: training loss is 0.349597, validation loss is 0.498692.\n",
      "epoch 260: training loss is 0.349183, validation loss is 0.498463.\n",
      "epoch 261: training loss is 0.348770, validation loss is 0.498236.\n",
      "epoch 262: training loss is 0.348360, validation loss is 0.498010.\n",
      "epoch 263: training loss is 0.347952, validation loss is 0.497786.\n",
      "epoch 264: training loss is 0.347546, validation loss is 0.497562.\n",
      "epoch 265: training loss is 0.347142, validation loss is 0.497340.\n",
      "epoch 266: training loss is 0.346740, validation loss is 0.497119.\n",
      "epoch 267: training loss is 0.346340, validation loss is 0.496899.\n",
      "epoch 268: training loss is 0.345942, validation loss is 0.496680.\n",
      "epoch 269: training loss is 0.345546, validation loss is 0.496462.\n",
      "epoch 270: training loss is 0.345153, validation loss is 0.496245.\n",
      "epoch 271: training loss is 0.344761, validation loss is 0.496030.\n",
      "epoch 272: training loss is 0.344371, validation loss is 0.495815.\n",
      "epoch 273: training loss is 0.343983, validation loss is 0.495602.\n",
      "epoch 274: training loss is 0.343597, validation loss is 0.495389.\n",
      "epoch 275: training loss is 0.343213, validation loss is 0.495178.\n",
      "epoch 276: training loss is 0.342831, validation loss is 0.494968.\n",
      "epoch 277: training loss is 0.342450, validation loss is 0.494759.\n",
      "epoch 278: training loss is 0.342072, validation loss is 0.494551.\n",
      "epoch 279: training loss is 0.341695, validation loss is 0.494344.\n",
      "epoch 280: training loss is 0.341321, validation loss is 0.494138.\n",
      "epoch 281: training loss is 0.340948, validation loss is 0.493933.\n",
      "epoch 282: training loss is 0.340577, validation loss is 0.493729.\n",
      "epoch 283: training loss is 0.340208, validation loss is 0.493526.\n",
      "epoch 284: training loss is 0.339841, validation loss is 0.493325.\n",
      "epoch 285: training loss is 0.339475, validation loss is 0.493124.\n",
      "epoch 286: training loss is 0.339111, validation loss is 0.492924.\n",
      "epoch 287: training loss is 0.338749, validation loss is 0.492725.\n",
      "epoch 288: training loss is 0.338389, validation loss is 0.492528.\n",
      "epoch 289: training loss is 0.338031, validation loss is 0.492331.\n",
      "epoch 290: training loss is 0.337674, validation loss is 0.492135.\n",
      "epoch 291: training loss is 0.337319, validation loss is 0.491940.\n",
      "epoch 292: training loss is 0.336966, validation loss is 0.491746.\n",
      "epoch 293: training loss is 0.336614, validation loss is 0.491553.\n",
      "epoch 294: training loss is 0.336264, validation loss is 0.491361.\n",
      "epoch 295: training loss is 0.335916, validation loss is 0.491170.\n",
      "epoch 296: training loss is 0.335569, validation loss is 0.490980.\n",
      "epoch 297: training loss is 0.335224, validation loss is 0.490791.\n",
      "epoch 298: training loss is 0.334881, validation loss is 0.490603.\n",
      "epoch 299: training loss is 0.334539, validation loss is 0.490415.\n"
     ]
    }
   ],
   "source": [
    "# Under BOW-tfidf situation, training the model by using the best parameters\n",
    "weights_bow_tfidf, tra_loss_bow_tfidf, val_loss_bow_tfidf = SGD(train_tfidf_mat, train_label, dev_tfidf_mat, \n",
    "                                                                dev_label, \n",
    "                                                                lr = best_parameter_bow_tfidf[0], \n",
    "                                                                alpha = best_parameter_bow_tfidf[1],\n",
    "                                                                epochs = best_parameter_bow_tfidf[2],\n",
    "                                                                tolerance = best_parameter_bow_tfidf[3], \n",
    "                                                                print_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the figure to check the training and validation loss during the training process\n",
    "\n",
    "plot_loss_history(tra_loss_bow_tfidf, val_loss_bow_tfidf, \"BOW-TFIDF(fig-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the loss-epoch figure for model of BOW-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through observing the figure 2, we could determine that the model for BOW-TFIDF is trained well.\n",
    "\n",
    "1. The validation loss is low (about 0.488), the validation loss line begins to be flat at the end of training, and the test accuracy from the following evaluation is high (about 85.5%). Therefore, the model is not underfitted.\n",
    "\n",
    "2. The difference between the final validation loss and final training loss on the figure 2 is small (about 0.15) and the validation loss line doesn't have the tend to rise. Therefore, the model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865\n",
      "Precision: 0.8509615384615384\n",
      "Recall: 0.885\n",
      "F1-Score: 0.8676470588235293\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy, precision, recall, and f1_score to evaluate the model under BOW-tfidf situation\n",
    "\n",
    "preds_bow_tfidf = predict_class(test_tfidf_mat, weights_bow_tfidf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_label, preds_bow_tfidf))\n",
    "print('Precision:', precision_score(test_label, preds_bow_tfidf))\n",
    "print('Recall:', recall_score(test_label, preds_bow_tfidf))\n",
    "print('F1-Score:', f1_score(test_label, preds_bow_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "worst\n",
      "boring\n",
      "supposed\n",
      "stupid\n",
      "harry\n",
      "ridiculous\n",
      "unfortunately\n",
      "nothing\n",
      "plot\n"
     ]
    }
   ],
   "source": [
    "# print top 10 words for negative class and positive class respectively.\n",
    "\n",
    "top_neg_bow_tfidf = weights_bow_tfidf.reshape(1, -1).argsort()[0][:10]\n",
    "for i in top_neg_bow_tfidf:\n",
    "    print(id2word_bow[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "life\n",
      "world\n",
      "war\n",
      "perfectly\n",
      "hilarious\n",
      "perfect\n",
      "terrific\n",
      "cameron\n",
      "damon\n"
     ]
    }
   ],
   "source": [
    "top_pos_bow_tfidf = weights_bow_tfidf.reshape(1, -1).argsort()[0][::-1][:10]\n",
    "for i in top_pos_bow_tfidf:\n",
    "    print(id2word_bow[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the top-10 words for model of BOW-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing the above top-10 words, we could find the top-10 words for BOW-TFIDF model could make more sense. Almost all of them are useful to classify if a review is positive or negative.\n",
    "\n",
    "For example, the top-10 negative words include \"bad\", \"worst\", \"boring\", \"stupid\", \"ridiculous\", \"unfortunately\", and \"nothing\", and the top-10 positive words include \"great\", \"perfectly\", \"hilarious\", \"cameron\", \"damon\", and \"terrific\". All of these words are useful to classify the opinion of movies' reviews.\n",
    "\n",
    "Among them, some are generalized well and could be used in other areas, such as \"bad\", \"worst\", \"great\", \"perfectly\", and \"terrific\". You could describe a laptop or a food is \"bad\" or \"great\". However, some of them are not generalized well and couldn't be used in other areas such as a restaurant review. Because you could not describe the food is \"ridiculous\" and there is no food related to the movie director \"Cameron\" and actor \"Damon\".\n",
    "\n",
    "We should notice that some adjective words are restricted to be used as a classification feature. Because it might be not proper to describe a food with the adjective \"romantic\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how I choose hyperparameters for model of BOW-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I choose the hyperparameters (learning rate, alpha, epoch, tolerance) by testing the development(validation) dataset on several different set of hyperparameters. The set of hyperparameters which could help the model to achieve the highest validation accuracy and lowest validation loss are the best hyperparameters. I will randomly display several set of hyperparameters and the corresponding validation loss and validation accuracy.\n",
    "\n",
    "| lr | alpha |epoch  | tolerance | val_loss | val_accuracy |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 0.000010  | 0.100000 | 300  |  0.000100 | 0.490416  | 0.835000  |\n",
    "| 0.000010  |  0.500000| 300  |  0.000100  | 0.557859 | 0.795000  |\n",
    "| 0.000010  | 0.100000   |100  | 0.000100 | 0.561617  | 0.800000 |\n",
    "| 0.000001 | 0.100000  |  300  | 0.000100  | 0.632733  | 0.760000  |\n",
    "| 0.000010  | 0.100000   |  300 | 0.000010  |0.490415  | 0.835000 |\n",
    "\n",
    "From the above table, we could find when the lr = 1e-5, alpha = 0.1, epoch = 300, tolerance = 1e-5, the validation loss is lowest (0.49) and the validation accuracy is highest (0.835). Therefore, I choose this set of hyperparameters as my best hyperparameters for model of BOW-TFIDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: BOCN-COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation for BOCN-COUNT\n",
    "Because we don't have the vocabulary and vectorized matrix for BOCN-COUNT/TIFDF. We need to get the vocabulary and vectorized matrix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabular, document frequency dictionary, and counts dictionary for BOCN \n",
    "#     I choose the keep_topN = 4000 because if the keep_topN is too large, there will be too many words in vocabulary \n",
    "# for BOCN. It will slow down the training speed.\n",
    "\n",
    "#     I choose the ngram_range = (3, 5) because if the ngram_range value is too small, the character combinations \n",
    "# will be too short. At this time, different words could produce the same character combinations. For example, a\n",
    "# negative word \"worst\" contain character combinations \"wo\", but a positive word \"wonderful\" could also contain a\n",
    "# character combinations \"wo\". So, if the ngram_range value is too small, the short character combinations would be\n",
    "# confusing.\n",
    "#    Similarly, if ngram_range is too large, the character combinations would be too long. At that time, a long \n",
    "# character combination might only appear in one document. It will be also bad for model training.\n",
    "\n",
    "vocab_bocn, df_bocn, ngram_counts_bocn = get_vocab(X_raw = train_text, ngram_range = (3, 5), min_df = 0, \n",
    "                                                   keep_topN = 4000, stop_words = stop_words, char_ngrams = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     create 2 dictionaries: (1) id2word_bocn: keys = vocabulary ids, values = words (2) word2id_bocn: keys = words, \n",
    "# values = vocabulary ids for BOCN\n",
    "\n",
    "id2word_bocn = dict()\n",
    "word2id_bocn = dict()\n",
    "\n",
    "for i in range(len(vocab_bocn)):\n",
    "    id2word_bocn[i] = list(vocab_bocn)[i]\n",
    "    word2id_bocn[list(vocab_bocn)[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize three list for training texts, development texts, and test texts features of BOCN\n",
    "train_extracted_ngrams_bocn = []\n",
    "dev_extracted_ngrams_bocn = []\n",
    "test_extracted_ngrams_bocn = []\n",
    "\n",
    "# extract ngrams for training set\n",
    "for i in range(len(train_text)):\n",
    "    temp = extract_ngrams(x_raw = train_text[i], ngram_range = (3, 5), \n",
    "                          stop_words = stop_words, char_ngrams = True)\n",
    "    train_extracted_ngrams_bocn.append(temp)\n",
    "\n",
    "# extract ngrams for development set\n",
    "for i in range(len(dev_text)):\n",
    "    temp = extract_ngrams(x_raw = dev_text[i], ngram_range = (3, 5),\n",
    "                         stop_words = stop_words, char_ngrams = True)\n",
    "    dev_extracted_ngrams_bocn.append(temp)\n",
    "\n",
    "# extract ngrams for test set\n",
    "for i in range(len(test_text)):\n",
    "    temp = extract_ngrams(x_raw = test_text[i], ngram_range = (3, 5),\n",
    "                         stop_words = stop_words, char_ngrams = True)\n",
    "    test_extracted_ngrams_bocn.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain count vectors for each document in the train, developemnt and test set of BOCN\n",
    "train_count_mat_bocn = vectorise(train_extracted_ngrams_bocn, vocab_bocn, word2id_bocn)\n",
    "dev_count_mat_bocn = vectorise(dev_extracted_ngrams_bocn, vocab_bocn, word2id_bocn)\n",
    "test_count_mat_bocn = vectorise(test_extracted_ngrams_bocn, vocab_bocn, word2id_bocn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the inverted document frequencies values for words in vocab_bocn\n",
    "idfs_bocn = np.zeros((1, len(vocab_bocn)))\n",
    "for i in range(len(vocab_bocn)):\n",
    "    idfs_bocn[0, i] = np.log10(len(train_text)/df_bocn[list(vocab_bocn)[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the count vectors to tf.idf vectors\n",
    "\n",
    "train_tfidf_mat_bocn = train_count_mat_bocn * idfs_bocn\n",
    "dev_tfidf_mat_bocn = dev_count_mat_bocn * idfs_bocn\n",
    "test_tfidf_mat_bocn = test_count_mat_bocn * idfs_bocn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the feature matrices for BOCN. We could train the model and evaluate for BOCN-COUNT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tunning for the model of BOCN-COUNT\n",
    "\n",
    "Please don't count the running time of this hyperparameters tunning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.000010, alpha = 0.500000, epoch =  30, tolerance = 0.000010, val_loss = 0.493779, val_acc = 0.785000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch =  30, tolerance = 0.000000, val_loss = 0.493751, val_acc = 0.785000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.468004, val_acc = 0.770000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000000, val_loss = 0.469430, val_acc = 0.765000\n",
      "\n",
      "lr = 0.000010, alpha = 0.300000, epoch =  30, tolerance = 0.000010, val_loss = 0.491184, val_acc = 0.775000\n",
      "\n",
      "lr = 0.000010, alpha = 0.300000, epoch =  30, tolerance = 0.000000, val_loss = 0.491716, val_acc = 0.775000\n",
      "\n",
      "lr = 0.000010, alpha = 0.300000, epoch = 100, tolerance = 0.000010, val_loss = 0.473518, val_acc = 0.780000\n",
      "\n",
      "lr = 0.000010, alpha = 0.300000, epoch = 100, tolerance = 0.000000, val_loss = 0.468880, val_acc = 0.755000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch =  30, tolerance = 0.000010, val_loss = 0.611111, val_acc = 0.740000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch =  30, tolerance = 0.000000, val_loss = 0.611014, val_acc = 0.740000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.546285, val_acc = 0.760000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000000, val_loss = 0.546292, val_acc = 0.775000\n",
      "\n",
      "lr = 0.000001, alpha = 0.300000, epoch =  30, tolerance = 0.000010, val_loss = 0.610593, val_acc = 0.705000\n",
      "\n",
      "lr = 0.000001, alpha = 0.300000, epoch =  30, tolerance = 0.000000, val_loss = 0.610604, val_acc = 0.710000\n",
      "\n",
      "lr = 0.000001, alpha = 0.300000, epoch = 100, tolerance = 0.000010, val_loss = 0.545247, val_acc = 0.760000\n",
      "\n",
      "lr = 0.000001, alpha = 0.300000, epoch = 100, tolerance = 0.000000, val_loss = 0.545424, val_acc = 0.775000\n",
      "\n",
      "The best parameters are: lr = 1e-05, alpha = 0.5, epoch = 100, tolerance = 1e-05\n"
     ]
    }
   ],
   "source": [
    "# choose the best hyperparameters for BOCN-COUNT\n",
    "\n",
    "best_parameter_bocn_c = choose_hyper(train_count_mat_bocn, train_label, dev_count_mat_bocn, dev_label, \n",
    "                              lr_candidate = [1e-5, 1e-6], alpha_candidate = [5*1e-1, 3*1e-1], \n",
    "                              epoch_candidate = [30, 100], tolerance_candidate = [1e-5, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of hyperparameters tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model of BOCN-COUNT by using the best hyperparameters we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: training loss is 0.645586, validation loss is 0.653823.\n",
      "epoch   1: training loss is 0.614293, validation loss is 0.631708.\n",
      "epoch   2: training loss is 0.586714, validation loss is 0.610302.\n",
      "epoch   3: training loss is 0.566642, validation loss is 0.598398.\n",
      "epoch   4: training loss is 0.545757, validation loss is 0.583770.\n",
      "epoch   5: training loss is 0.529950, validation loss is 0.573969.\n",
      "epoch   6: training loss is 0.516108, validation loss is 0.566292.\n",
      "epoch   7: training loss is 0.503049, validation loss is 0.558482.\n",
      "epoch   8: training loss is 0.491771, validation loss is 0.552827.\n",
      "epoch   9: training loss is 0.480385, validation loss is 0.546071.\n",
      "epoch  10: training loss is 0.471549, validation loss is 0.542238.\n",
      "epoch  11: training loss is 0.462171, validation loss is 0.537103.\n",
      "epoch  12: training loss is 0.454674, validation loss is 0.533925.\n",
      "epoch  13: training loss is 0.445727, validation loss is 0.528402.\n",
      "epoch  14: training loss is 0.438677, validation loss is 0.524381.\n",
      "epoch  15: training loss is 0.432102, validation loss is 0.521093.\n",
      "epoch  16: training loss is 0.425272, validation loss is 0.518268.\n",
      "epoch  17: training loss is 0.419327, validation loss is 0.515265.\n",
      "epoch  18: training loss is 0.415314, validation loss is 0.513684.\n",
      "epoch  19: training loss is 0.408523, validation loss is 0.511294.\n",
      "epoch  20: training loss is 0.405593, validation loss is 0.509705.\n",
      "epoch  21: training loss is 0.398580, validation loss is 0.506307.\n",
      "epoch  22: training loss is 0.393902, validation loss is 0.504439.\n",
      "epoch  23: training loss is 0.389691, validation loss is 0.502595.\n",
      "epoch  24: training loss is 0.386981, validation loss is 0.504300.\n",
      "epoch  25: training loss is 0.381313, validation loss is 0.499842.\n",
      "epoch  26: training loss is 0.377982, validation loss is 0.499523.\n",
      "epoch  27: training loss is 0.373824, validation loss is 0.497063.\n",
      "epoch  28: training loss is 0.376340, validation loss is 0.504750.\n",
      "epoch  29: training loss is 0.368351, validation loss is 0.494504.\n",
      "epoch  30: training loss is 0.364820, validation loss is 0.493256.\n",
      "epoch  31: training loss is 0.360721, validation loss is 0.491671.\n",
      "epoch  32: training loss is 0.357808, validation loss is 0.490558.\n",
      "epoch  33: training loss is 0.354861, validation loss is 0.491082.\n",
      "epoch  34: training loss is 0.353937, validation loss is 0.489647.\n",
      "epoch  35: training loss is 0.350180, validation loss is 0.488061.\n",
      "epoch  36: training loss is 0.347680, validation loss is 0.490138.\n",
      "epoch  37: training loss is 0.343774, validation loss is 0.486460.\n",
      "epoch  38: training loss is 0.341363, validation loss is 0.485850.\n",
      "epoch  39: training loss is 0.338909, validation loss is 0.484894.\n",
      "epoch  40: training loss is 0.336730, validation loss is 0.483725.\n",
      "epoch  41: training loss is 0.334443, validation loss is 0.483988.\n",
      "epoch  42: training loss is 0.334598, validation loss is 0.487704.\n",
      "epoch  43: training loss is 0.330052, validation loss is 0.482413.\n",
      "epoch  44: training loss is 0.328038, validation loss is 0.481417.\n",
      "epoch  45: training loss is 0.327500, validation loss is 0.480951.\n",
      "epoch  46: training loss is 0.324107, validation loss is 0.480244.\n",
      "epoch  47: training loss is 0.322515, validation loss is 0.479550.\n",
      "epoch  48: training loss is 0.321354, validation loss is 0.479254.\n",
      "epoch  49: training loss is 0.322244, validation loss is 0.480334.\n",
      "epoch  50: training loss is 0.317519, validation loss is 0.480566.\n",
      "epoch  51: training loss is 0.322191, validation loss is 0.482318.\n",
      "epoch  52: training loss is 0.313550, validation loss is 0.477496.\n",
      "epoch  53: training loss is 0.312004, validation loss is 0.477907.\n",
      "epoch  54: training loss is 0.310374, validation loss is 0.477252.\n",
      "epoch  55: training loss is 0.309040, validation loss is 0.476096.\n",
      "epoch  56: training loss is 0.307322, validation loss is 0.476378.\n",
      "epoch  57: training loss is 0.305979, validation loss is 0.475662.\n",
      "epoch  58: training loss is 0.304598, validation loss is 0.475257.\n",
      "epoch  59: training loss is 0.303143, validation loss is 0.475842.\n",
      "epoch  60: training loss is 0.301713, validation loss is 0.475157.\n",
      "epoch  61: training loss is 0.300408, validation loss is 0.474498.\n",
      "epoch  62: training loss is 0.299405, validation loss is 0.475767.\n",
      "epoch  63: training loss is 0.297836, validation loss is 0.474230.\n",
      "epoch  64: training loss is 0.296634, validation loss is 0.473535.\n",
      "epoch  65: training loss is 0.295740, validation loss is 0.474924.\n",
      "epoch  66: training loss is 0.294432, validation loss is 0.474360.\n",
      "epoch  67: training loss is 0.294742, validation loss is 0.472946.\n",
      "epoch  68: training loss is 0.292280, validation loss is 0.472362.\n",
      "epoch  69: training loss is 0.290960, validation loss is 0.473468.\n",
      "epoch  70: training loss is 0.291022, validation loss is 0.471995.\n",
      "epoch  71: training loss is 0.289962, validation loss is 0.471683.\n",
      "epoch  72: training loss is 0.287720, validation loss is 0.471593.\n",
      "epoch  73: training loss is 0.286962, validation loss is 0.471274.\n",
      "epoch  74: training loss is 0.285700, validation loss is 0.472029.\n",
      "epoch  75: training loss is 0.284696, validation loss is 0.471278.\n",
      "epoch  76: training loss is 0.284569, validation loss is 0.473928.\n",
      "epoch  77: training loss is 0.283320, validation loss is 0.473083.\n",
      "epoch  78: training loss is 0.282190, validation loss is 0.470487.\n",
      "epoch  79: training loss is 0.280997, validation loss is 0.470969.\n",
      "epoch  80: training loss is 0.280412, validation loss is 0.470127.\n",
      "epoch  81: training loss is 0.279317, validation loss is 0.471102.\n",
      "epoch  82: training loss is 0.278902, validation loss is 0.469691.\n",
      "epoch  83: training loss is 0.277584, validation loss is 0.470065.\n",
      "epoch  84: training loss is 0.277148, validation loss is 0.471675.\n",
      "epoch  85: training loss is 0.276673, validation loss is 0.472343.\n",
      "epoch  86: training loss is 0.276528, validation loss is 0.473538.\n",
      "epoch  87: training loss is 0.274412, validation loss is 0.469801.\n",
      "epoch  88: training loss is 0.273817, validation loss is 0.469171.\n",
      "epoch  89: training loss is 0.273869, validation loss is 0.472470.\n",
      "epoch  90: training loss is 0.272199, validation loss is 0.469449.\n",
      "epoch  91: training loss is 0.272442, validation loss is 0.468756.\n",
      "epoch  92: training loss is 0.270804, validation loss is 0.469252.\n",
      "epoch  93: training loss is 0.270117, validation loss is 0.469136.\n",
      "epoch  94: training loss is 0.270152, validation loss is 0.471565.\n",
      "epoch  95: training loss is 0.269793, validation loss is 0.468367.\n",
      "epoch  96: training loss is 0.268366, validation loss is 0.470058.\n",
      "epoch  97: training loss is 0.267636, validation loss is 0.468216.\n",
      "epoch  98: training loss is 0.267125, validation loss is 0.469952.\n",
      "epoch  99: training loss is 0.266448, validation loss is 0.468028.\n"
     ]
    }
   ],
   "source": [
    "# Under BOCN-COUNT situation, training the model by using the best parameters\n",
    "weights_bocn_c, tra_loss_bocn_c, val_loss_bocn_c = SGD(train_count_mat_bocn, train_label, dev_count_mat_bocn, \n",
    "                                                       dev_label, \n",
    "                                                       lr = best_parameter_bocn_c[0], \n",
    "                                                       alpha = best_parameter_bocn_c[1],\n",
    "                                                       epochs = best_parameter_bocn_c[2],\n",
    "                                                       tolerance = best_parameter_bocn_c[3], \n",
    "                                                       print_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the figure to check the training and validation loss during the training process\n",
    "\n",
    "plot_loss_history(tra_loss_bocn_c, val_loss_bocn_c, \"BOCN-COUNT(fig-3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the loss-epoch figure for model of BOCN-COUNT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From figure 3, we could know the model for BOCN-COUNT is trained well.\n",
    "\n",
    "1. Because the final validation loss is low enough (about 0.46) and the validation loss tends to be flat from declining at the end of training, the model is not underfitted.\n",
    "\n",
    "2. The difference between the final training loss and the final validation loss is small (about 0.2) and the validation loss doesn't have the overall tendency to rise. Therefore, the model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.8232323232323232\n",
      "Recall: 0.815\n",
      "F1-Score: 0.8190954773869347\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy, precision, recall, and f1_score to evaluate the model under BOCN-count situation\n",
    "\n",
    "preds_bocn_c = predict_class(test_count_mat_bocn, weights_bocn_c)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_label, preds_bocn_c))\n",
    "print('Precision:', precision_score(test_label, preds_bocn_c))\n",
    "print('Recall:', recall_score(test_label, preds_bocn_c))\n",
    "print('F1-Score:', f1_score(test_label, preds_bocn_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "ors\n",
      "esa\n",
      "wors\n",
      "tal\n",
      "nly\n",
      "was\n",
      "only\n",
      "hen\n",
      "plo\n"
     ]
    }
   ],
   "source": [
    "# print top 10 words for negative class and positive class respectively.\n",
    "\n",
    "top_neg_bocn_c = weights_bocn_c.reshape(1, -1).argsort()[0][:10]\n",
    "for i in top_neg_bocn_c:\n",
    "    print(id2word_bocn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ese\n",
      "gre\n",
      "ral\n",
      "erf\n",
      "tru\n",
      "grea\n",
      "lar\n",
      "ood\n",
      "eca\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "top_pos_bocn_c = weights_bocn_c.reshape(1, -1).argsort()[0][::-1][:10]\n",
    "for i in top_pos_bocn_c:\n",
    "    print(id2word_bocn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the top-10 words for model of BOCN-COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-10 character combinations of model for BOCN-COUNT make sense for sentiment classification task.\n",
    "\n",
    "For example, \"wors\", \"ors\" is part of the word \"worst\", and \"bad\" is equal to the word \"bad\". Then, \"gre\", \"grea\" is part of the word \"great\", and \"erf\" is part of the word \"perfect\". These words could be used to describe a movie.\n",
    "\n",
    "Then, I think these character combinations are generalized well in other domain like restaurant review. Because it is reasonable to say \"this is the worst food I have ever eaten\" and \"this is a great meal\".\n",
    "\n",
    "For a restraurant review domain, the classifier may pick up character combinations like \"delic\"(part of delicious), \"tast\"(part of \"tasty\")...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how I choose hyperparameters for model of BOCN-COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I choose the hyperparameters (learning rate, alpha, epoch, tolerance) by testing the development(validation) dataset on several different set of hyperparameters. The set of hyperparameters which could help the model to achieve the highest validation accuracy and lowest validation loss are the best hyperparameters. I will randomly display several set of hyperparameters and the corresponding validation loss and validation accuracy.\n",
    "\n",
    "| lr | alpha |epoch  | tolerance | val_loss | val_accuracy |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 0.000010  | 0.500000 | 100  |  0.000010 | 0.468004 | 0.790000  |\n",
    "| 0.000010  |  0.300000| 100  |  0.000010  | 0.473518 | 0.780000  |\n",
    "| 0.000010  | 0.500000   |30  | 0.000010 | 0.493779  | 0.785000 |\n",
    "| 0.000001 | 0.500000  |  100  | 0.000010  | 0.546285  | 0.760000 |\n",
    "| 0.000010  | 0.500000   |  100 | 0  |0.469430  | 0.765000 |\n",
    "\n",
    "From the above table, we could find when the lr = 1e-5, alpha = 0.5, epoch = 100, tolerance = 1e-5, the validation loss is lowest (0.468) and the validation accuracy is highest (0.79). Therefore, I choose this set of hyperparameters as my best hyperparameters for model of BOCN-COUNT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: BOCN-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluate for BOCN-tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hyperparameters tuning part for the model of BOCN-TFIDF\n",
    "\n",
    "Please don't count the running time of this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.558239, val_acc = 0.775000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.558246, val_acc = 0.775000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.543069, val_acc = 0.780000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.536460, val_acc = 0.775000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.536973, val_acc = 0.780000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.536955, val_acc = 0.785000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.503381, val_acc = 0.770000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.489931, val_acc = 0.765000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.656220, val_acc = 0.655000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.656217, val_acc = 0.650000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.613613, val_acc = 0.735000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.613611, val_acc = 0.735000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.655336, val_acc = 0.660000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.655330, val_acc = 0.660000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.608475, val_acc = 0.735000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.608474, val_acc = 0.735000\n",
      "\n",
      "The best parameters are: lr = 1e-05, alpha = 0.1, epoch = 300, tolerance = 1e-05\n"
     ]
    }
   ],
   "source": [
    "# choose the best hyperparameters for BOCN-tfidf\n",
    "\n",
    "best_parameter_bocn_tfidf = choose_hyper(train_tfidf_mat_bocn, train_label, dev_tfidf_mat_bocn, dev_label, \n",
    "                              lr_candidate = [1e-5, 1e-6], alpha_candidate = [5*1e-1, 1e-1], \n",
    "                              epoch_candidate = [100, 300], tolerance_candidate = [1e-4, 1e-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of the hyperparameters tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model of BOCN-TFIDF by using the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: training loss is 0.685852, validation loss is 0.687071.\n",
      "epoch   1: training loss is 0.679900, validation loss is 0.682709.\n",
      "epoch   2: training loss is 0.674313, validation loss is 0.678848.\n",
      "epoch   3: training loss is 0.668899, validation loss is 0.675120.\n",
      "epoch   4: training loss is 0.663651, validation loss is 0.671512.\n",
      "epoch   5: training loss is 0.658564, validation loss is 0.668081.\n",
      "epoch   6: training loss is 0.653614, validation loss is 0.664702.\n",
      "epoch   7: training loss is 0.648803, validation loss is 0.661434.\n",
      "epoch   8: training loss is 0.644118, validation loss is 0.658320.\n",
      "epoch   9: training loss is 0.639563, validation loss is 0.655235.\n",
      "epoch  10: training loss is 0.635116, validation loss is 0.652299.\n",
      "epoch  11: training loss is 0.630786, validation loss is 0.649474.\n",
      "epoch  12: training loss is 0.626570, validation loss is 0.646689.\n",
      "epoch  13: training loss is 0.622453, validation loss is 0.643919.\n",
      "epoch  14: training loss is 0.618443, validation loss is 0.641277.\n",
      "epoch  15: training loss is 0.614539, validation loss is 0.638744.\n",
      "epoch  16: training loss is 0.610704, validation loss is 0.636128.\n",
      "epoch  17: training loss is 0.606973, validation loss is 0.633659.\n",
      "epoch  18: training loss is 0.603327, validation loss is 0.631274.\n",
      "epoch  19: training loss is 0.599767, validation loss is 0.628948.\n",
      "epoch  20: training loss is 0.596289, validation loss is 0.626694.\n",
      "epoch  21: training loss is 0.592890, validation loss is 0.624483.\n",
      "epoch  22: training loss is 0.589553, validation loss is 0.622241.\n",
      "epoch  23: training loss is 0.586320, validation loss is 0.620083.\n",
      "epoch  24: training loss is 0.583112, validation loss is 0.618070.\n",
      "epoch  25: training loss is 0.579993, validation loss is 0.616043.\n",
      "epoch  26: training loss is 0.576947, validation loss is 0.614041.\n",
      "epoch  27: training loss is 0.573950, validation loss is 0.612152.\n",
      "epoch  28: training loss is 0.571019, validation loss is 0.610241.\n",
      "epoch  29: training loss is 0.568151, validation loss is 0.608386.\n",
      "epoch  30: training loss is 0.565335, validation loss is 0.606585.\n",
      "epoch  31: training loss is 0.562571, validation loss is 0.604834.\n",
      "epoch  32: training loss is 0.559872, validation loss is 0.603158.\n",
      "epoch  33: training loss is 0.557211, validation loss is 0.601456.\n",
      "epoch  34: training loss is 0.554600, validation loss is 0.599778.\n",
      "epoch  35: training loss is 0.552043, validation loss is 0.598174.\n",
      "epoch  36: training loss is 0.549532, validation loss is 0.596578.\n",
      "epoch  37: training loss is 0.547081, validation loss is 0.595026.\n",
      "epoch  38: training loss is 0.544645, validation loss is 0.593511.\n",
      "epoch  39: training loss is 0.542262, validation loss is 0.592030.\n",
      "epoch  40: training loss is 0.539924, validation loss is 0.590575.\n",
      "epoch  41: training loss is 0.537624, validation loss is 0.589143.\n",
      "epoch  42: training loss is 0.535364, validation loss is 0.587739.\n",
      "epoch  43: training loss is 0.533159, validation loss is 0.586375.\n",
      "epoch  44: training loss is 0.530965, validation loss is 0.585023.\n",
      "epoch  45: training loss is 0.528812, validation loss is 0.583702.\n",
      "epoch  46: training loss is 0.526691, validation loss is 0.582403.\n",
      "epoch  47: training loss is 0.524614, validation loss is 0.581136.\n",
      "epoch  48: training loss is 0.522586, validation loss is 0.579911.\n",
      "epoch  49: training loss is 0.520561, validation loss is 0.578678.\n",
      "epoch  50: training loss is 0.518560, validation loss is 0.577456.\n",
      "epoch  51: training loss is 0.516602, validation loss is 0.576271.\n",
      "epoch  52: training loss is 0.514681, validation loss is 0.575120.\n",
      "epoch  53: training loss is 0.512781, validation loss is 0.573976.\n",
      "epoch  54: training loss is 0.510909, validation loss is 0.572848.\n",
      "epoch  55: training loss is 0.509070, validation loss is 0.571757.\n",
      "epoch  56: training loss is 0.507257, validation loss is 0.570682.\n",
      "epoch  57: training loss is 0.505477, validation loss is 0.569638.\n",
      "epoch  58: training loss is 0.503701, validation loss is 0.568570.\n",
      "epoch  59: training loss is 0.501959, validation loss is 0.567534.\n",
      "epoch  60: training loss is 0.500244, validation loss is 0.566527.\n",
      "epoch  61: training loss is 0.498553, validation loss is 0.565538.\n",
      "epoch  62: training loss is 0.496892, validation loss is 0.564582.\n",
      "epoch  63: training loss is 0.495243, validation loss is 0.563613.\n",
      "epoch  64: training loss is 0.493615, validation loss is 0.562658.\n",
      "epoch  65: training loss is 0.492011, validation loss is 0.561726.\n",
      "epoch  66: training loss is 0.490428, validation loss is 0.560803.\n",
      "epoch  67: training loss is 0.488871, validation loss is 0.559921.\n",
      "epoch  68: training loss is 0.487345, validation loss is 0.559075.\n",
      "epoch  69: training loss is 0.485811, validation loss is 0.558173.\n",
      "epoch  70: training loss is 0.484304, validation loss is 0.557302.\n",
      "epoch  71: training loss is 0.482815, validation loss is 0.556414.\n",
      "epoch  72: training loss is 0.481350, validation loss is 0.555596.\n",
      "epoch  73: training loss is 0.479902, validation loss is 0.554759.\n",
      "epoch  74: training loss is 0.478484, validation loss is 0.553915.\n",
      "epoch  75: training loss is 0.477062, validation loss is 0.553130.\n",
      "epoch  76: training loss is 0.475670, validation loss is 0.552335.\n",
      "epoch  77: training loss is 0.474283, validation loss is 0.551595.\n",
      "epoch  78: training loss is 0.472924, validation loss is 0.550799.\n",
      "epoch  79: training loss is 0.471572, validation loss is 0.550058.\n",
      "epoch  80: training loss is 0.470242, validation loss is 0.549364.\n",
      "epoch  81: training loss is 0.468923, validation loss is 0.548628.\n",
      "epoch  82: training loss is 0.467620, validation loss is 0.547885.\n",
      "epoch  83: training loss is 0.466333, validation loss is 0.547185.\n",
      "epoch  84: training loss is 0.465060, validation loss is 0.546469.\n",
      "epoch  85: training loss is 0.463801, validation loss is 0.545780.\n",
      "epoch  86: training loss is 0.462557, validation loss is 0.545079.\n",
      "epoch  87: training loss is 0.461325, validation loss is 0.544412.\n",
      "epoch  88: training loss is 0.460110, validation loss is 0.543791.\n",
      "epoch  89: training loss is 0.458901, validation loss is 0.543115.\n",
      "epoch  90: training loss is 0.457712, validation loss is 0.542496.\n",
      "epoch  91: training loss is 0.456530, validation loss is 0.541843.\n",
      "epoch  92: training loss is 0.455365, validation loss is 0.541150.\n",
      "epoch  93: training loss is 0.454206, validation loss is 0.540550.\n",
      "epoch  94: training loss is 0.453064, validation loss is 0.539931.\n",
      "epoch  95: training loss is 0.451932, validation loss is 0.539367.\n",
      "epoch  96: training loss is 0.450811, validation loss is 0.538753.\n",
      "epoch  97: training loss is 0.449704, validation loss is 0.538131.\n",
      "epoch  98: training loss is 0.448605, validation loss is 0.537585.\n",
      "epoch  99: training loss is 0.447519, validation loss is 0.537017.\n",
      "epoch 100: training loss is 0.446443, validation loss is 0.536457.\n",
      "epoch 101: training loss is 0.445378, validation loss is 0.535902.\n",
      "epoch 102: training loss is 0.444322, validation loss is 0.535314.\n",
      "epoch 103: training loss is 0.443284, validation loss is 0.534719.\n",
      "epoch 104: training loss is 0.442258, validation loss is 0.534153.\n",
      "epoch 105: training loss is 0.441220, validation loss is 0.533679.\n",
      "epoch 106: training loss is 0.440204, validation loss is 0.533162.\n",
      "epoch 107: training loss is 0.439199, validation loss is 0.532673.\n",
      "epoch 108: training loss is 0.438204, validation loss is 0.532139.\n",
      "epoch 109: training loss is 0.437218, validation loss is 0.531610.\n",
      "epoch 110: training loss is 0.436241, validation loss is 0.531095.\n",
      "epoch 111: training loss is 0.435273, validation loss is 0.530592.\n",
      "epoch 112: training loss is 0.434312, validation loss is 0.530123.\n",
      "epoch 113: training loss is 0.433361, validation loss is 0.529665.\n",
      "epoch 114: training loss is 0.432419, validation loss is 0.529159.\n",
      "epoch 115: training loss is 0.431487, validation loss is 0.528734.\n",
      "epoch 116: training loss is 0.430561, validation loss is 0.528264.\n",
      "epoch 117: training loss is 0.429643, validation loss is 0.527785.\n",
      "epoch 118: training loss is 0.428734, validation loss is 0.527300.\n",
      "epoch 119: training loss is 0.427834, validation loss is 0.526829.\n",
      "epoch 120: training loss is 0.426940, validation loss is 0.526406.\n",
      "epoch 121: training loss is 0.426057, validation loss is 0.526011.\n",
      "epoch 122: training loss is 0.425177, validation loss is 0.525544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123: training loss is 0.424307, validation loss is 0.525084.\n",
      "epoch 124: training loss is 0.423450, validation loss is 0.524596.\n",
      "epoch 125: training loss is 0.422593, validation loss is 0.524183.\n",
      "epoch 126: training loss is 0.421742, validation loss is 0.523859.\n",
      "epoch 127: training loss is 0.420900, validation loss is 0.523426.\n",
      "epoch 128: training loss is 0.420068, validation loss is 0.522951.\n",
      "epoch 129: training loss is 0.419239, validation loss is 0.522566.\n",
      "epoch 130: training loss is 0.418419, validation loss is 0.522171.\n",
      "epoch 131: training loss is 0.417605, validation loss is 0.521789.\n",
      "epoch 132: training loss is 0.416798, validation loss is 0.521381.\n",
      "epoch 133: training loss is 0.415997, validation loss is 0.521031.\n",
      "epoch 134: training loss is 0.415204, validation loss is 0.520658.\n",
      "epoch 135: training loss is 0.414416, validation loss is 0.520246.\n",
      "epoch 136: training loss is 0.413635, validation loss is 0.519879.\n",
      "epoch 137: training loss is 0.412860, validation loss is 0.519505.\n",
      "epoch 138: training loss is 0.412094, validation loss is 0.519075.\n",
      "epoch 139: training loss is 0.411333, validation loss is 0.518695.\n",
      "epoch 140: training loss is 0.410571, validation loss is 0.518420.\n",
      "epoch 141: training loss is 0.409821, validation loss is 0.518085.\n",
      "epoch 142: training loss is 0.409075, validation loss is 0.517717.\n",
      "epoch 143: training loss is 0.408340, validation loss is 0.517430.\n",
      "epoch 144: training loss is 0.407613, validation loss is 0.517139.\n",
      "epoch 145: training loss is 0.406884, validation loss is 0.516790.\n",
      "epoch 146: training loss is 0.406155, validation loss is 0.516400.\n",
      "epoch 147: training loss is 0.405440, validation loss is 0.516085.\n",
      "epoch 148: training loss is 0.404726, validation loss is 0.515732.\n",
      "epoch 149: training loss is 0.404017, validation loss is 0.515372.\n",
      "epoch 150: training loss is 0.403317, validation loss is 0.515055.\n",
      "epoch 151: training loss is 0.402626, validation loss is 0.514785.\n",
      "epoch 152: training loss is 0.401928, validation loss is 0.514384.\n",
      "epoch 153: training loss is 0.401244, validation loss is 0.513981.\n",
      "epoch 154: training loss is 0.400563, validation loss is 0.513678.\n",
      "epoch 155: training loss is 0.399888, validation loss is 0.513355.\n",
      "epoch 156: training loss is 0.399215, validation loss is 0.513107.\n",
      "epoch 157: training loss is 0.398552, validation loss is 0.512730.\n",
      "epoch 158: training loss is 0.397889, validation loss is 0.512521.\n",
      "epoch 159: training loss is 0.397232, validation loss is 0.512206.\n",
      "epoch 160: training loss is 0.396580, validation loss is 0.511872.\n",
      "epoch 161: training loss is 0.395934, validation loss is 0.511568.\n",
      "epoch 162: training loss is 0.395291, validation loss is 0.511334.\n",
      "epoch 163: training loss is 0.394653, validation loss is 0.511044.\n",
      "epoch 164: training loss is 0.394019, validation loss is 0.510760.\n",
      "epoch 165: training loss is 0.393390, validation loss is 0.510455.\n",
      "epoch 166: training loss is 0.392765, validation loss is 0.510177.\n",
      "epoch 167: training loss is 0.392146, validation loss is 0.509853.\n",
      "epoch 168: training loss is 0.391528, validation loss is 0.509631.\n",
      "epoch 169: training loss is 0.390918, validation loss is 0.509393.\n",
      "epoch 170: training loss is 0.390312, validation loss is 0.509163.\n",
      "epoch 171: training loss is 0.389706, validation loss is 0.508853.\n",
      "epoch 172: training loss is 0.389106, validation loss is 0.508563.\n",
      "epoch 173: training loss is 0.388511, validation loss is 0.508248.\n",
      "epoch 174: training loss is 0.387921, validation loss is 0.507966.\n",
      "epoch 175: training loss is 0.387332, validation loss is 0.507777.\n",
      "epoch 176: training loss is 0.386749, validation loss is 0.507553.\n",
      "epoch 177: training loss is 0.386171, validation loss is 0.507337.\n",
      "epoch 178: training loss is 0.385600, validation loss is 0.507134.\n",
      "epoch 179: training loss is 0.385023, validation loss is 0.506825.\n",
      "epoch 180: training loss is 0.384455, validation loss is 0.506583.\n",
      "epoch 181: training loss is 0.383893, validation loss is 0.506377.\n",
      "epoch 182: training loss is 0.383332, validation loss is 0.506136.\n",
      "epoch 183: training loss is 0.382771, validation loss is 0.505835.\n",
      "epoch 184: training loss is 0.382223, validation loss is 0.505670.\n",
      "epoch 185: training loss is 0.381669, validation loss is 0.505396.\n",
      "epoch 186: training loss is 0.381119, validation loss is 0.505083.\n",
      "epoch 187: training loss is 0.380576, validation loss is 0.504818.\n",
      "epoch 188: training loss is 0.380037, validation loss is 0.504581.\n",
      "epoch 189: training loss is 0.379501, validation loss is 0.504407.\n",
      "epoch 190: training loss is 0.378968, validation loss is 0.504191.\n",
      "epoch 191: training loss is 0.378439, validation loss is 0.503976.\n",
      "epoch 192: training loss is 0.377914, validation loss is 0.503774.\n",
      "epoch 193: training loss is 0.377389, validation loss is 0.503476.\n",
      "epoch 194: training loss is 0.376870, validation loss is 0.503239.\n",
      "epoch 195: training loss is 0.376355, validation loss is 0.503108.\n",
      "epoch 196: training loss is 0.375841, validation loss is 0.502865.\n",
      "epoch 197: training loss is 0.375331, validation loss is 0.502665.\n",
      "epoch 198: training loss is 0.374826, validation loss is 0.502488.\n",
      "epoch 199: training loss is 0.374320, validation loss is 0.502192.\n",
      "epoch 200: training loss is 0.373820, validation loss is 0.501983.\n",
      "epoch 201: training loss is 0.373323, validation loss is 0.501756.\n",
      "epoch 202: training loss is 0.372829, validation loss is 0.501528.\n",
      "epoch 203: training loss is 0.372339, validation loss is 0.501304.\n",
      "epoch 204: training loss is 0.371848, validation loss is 0.501182.\n",
      "epoch 205: training loss is 0.371362, validation loss is 0.501011.\n",
      "epoch 206: training loss is 0.370880, validation loss is 0.500820.\n",
      "epoch 207: training loss is 0.370400, validation loss is 0.500596.\n",
      "epoch 208: training loss is 0.369923, validation loss is 0.500441.\n",
      "epoch 209: training loss is 0.369450, validation loss is 0.500264.\n",
      "epoch 210: training loss is 0.368977, validation loss is 0.499993.\n",
      "epoch 211: training loss is 0.368509, validation loss is 0.499795.\n",
      "epoch 212: training loss is 0.368043, validation loss is 0.499669.\n",
      "epoch 213: training loss is 0.367579, validation loss is 0.499446.\n",
      "epoch 214: training loss is 0.367119, validation loss is 0.499296.\n",
      "epoch 215: training loss is 0.366663, validation loss is 0.499145.\n",
      "epoch 216: training loss is 0.366206, validation loss is 0.498898.\n",
      "epoch 217: training loss is 0.365754, validation loss is 0.498734.\n",
      "epoch 218: training loss is 0.365305, validation loss is 0.498476.\n",
      "epoch 219: training loss is 0.364856, validation loss is 0.498347.\n",
      "epoch 220: training loss is 0.364412, validation loss is 0.498182.\n",
      "epoch 221: training loss is 0.363970, validation loss is 0.497996.\n",
      "epoch 222: training loss is 0.363530, validation loss is 0.497829.\n",
      "epoch 223: training loss is 0.363096, validation loss is 0.497575.\n",
      "epoch 224: training loss is 0.362658, validation loss is 0.497479.\n",
      "epoch 225: training loss is 0.362226, validation loss is 0.497298.\n",
      "epoch 226: training loss is 0.361797, validation loss is 0.497177.\n",
      "epoch 227: training loss is 0.361370, validation loss is 0.496939.\n",
      "epoch 228: training loss is 0.360944, validation loss is 0.496796.\n",
      "epoch 229: training loss is 0.360522, validation loss is 0.496661.\n",
      "epoch 230: training loss is 0.360101, validation loss is 0.496480.\n",
      "epoch 231: training loss is 0.359683, validation loss is 0.496343.\n",
      "epoch 232: training loss is 0.359268, validation loss is 0.496191.\n",
      "epoch 233: training loss is 0.358855, validation loss is 0.496022.\n",
      "epoch 234: training loss is 0.358444, validation loss is 0.495831.\n",
      "epoch 235: training loss is 0.358035, validation loss is 0.495702.\n",
      "epoch 236: training loss is 0.357628, validation loss is 0.495546.\n",
      "epoch 237: training loss is 0.357224, validation loss is 0.495362.\n",
      "epoch 238: training loss is 0.356822, validation loss is 0.495198.\n",
      "epoch 239: training loss is 0.356424, validation loss is 0.494996.\n",
      "epoch 240: training loss is 0.356026, validation loss is 0.494842.\n",
      "epoch 241: training loss is 0.355631, validation loss is 0.494687.\n",
      "epoch 242: training loss is 0.355236, validation loss is 0.494559.\n",
      "epoch 243: training loss is 0.354844, validation loss is 0.494499.\n",
      "epoch 244: training loss is 0.354454, validation loss is 0.494319.\n",
      "epoch 245: training loss is 0.354067, validation loss is 0.494156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 246: training loss is 0.353682, validation loss is 0.494014.\n",
      "epoch 247: training loss is 0.353299, validation loss is 0.493857.\n",
      "epoch 248: training loss is 0.352917, validation loss is 0.493732.\n",
      "epoch 249: training loss is 0.352539, validation loss is 0.493655.\n",
      "epoch 250: training loss is 0.352163, validation loss is 0.493374.\n",
      "epoch 251: training loss is 0.351788, validation loss is 0.493244.\n",
      "epoch 252: training loss is 0.351413, validation loss is 0.493162.\n",
      "epoch 253: training loss is 0.351042, validation loss is 0.493088.\n",
      "epoch 254: training loss is 0.350672, validation loss is 0.492946.\n",
      "epoch 255: training loss is 0.350305, validation loss is 0.492741.\n",
      "epoch 256: training loss is 0.349939, validation loss is 0.492647.\n",
      "epoch 257: training loss is 0.349575, validation loss is 0.492482.\n",
      "epoch 258: training loss is 0.349214, validation loss is 0.492355.\n",
      "epoch 259: training loss is 0.348854, validation loss is 0.492232.\n",
      "epoch 260: training loss is 0.348497, validation loss is 0.492152.\n",
      "epoch 261: training loss is 0.348140, validation loss is 0.491994.\n",
      "epoch 262: training loss is 0.347785, validation loss is 0.491845.\n",
      "epoch 263: training loss is 0.347434, validation loss is 0.491662.\n",
      "epoch 264: training loss is 0.347083, validation loss is 0.491540.\n",
      "epoch 265: training loss is 0.346734, validation loss is 0.491410.\n",
      "epoch 266: training loss is 0.346386, validation loss is 0.491332.\n",
      "epoch 267: training loss is 0.346041, validation loss is 0.491260.\n",
      "epoch 268: training loss is 0.345697, validation loss is 0.491074.\n",
      "epoch 269: training loss is 0.345356, validation loss is 0.490900.\n",
      "epoch 270: training loss is 0.345014, validation loss is 0.490846.\n",
      "epoch 271: training loss is 0.344676, validation loss is 0.490756.\n",
      "epoch 272: training loss is 0.344339, validation loss is 0.490634.\n",
      "epoch 273: training loss is 0.344003, validation loss is 0.490498.\n",
      "epoch 274: training loss is 0.343671, validation loss is 0.490430.\n",
      "epoch 275: training loss is 0.343340, validation loss is 0.490160.\n",
      "epoch 276: training loss is 0.343007, validation loss is 0.490098.\n",
      "epoch 277: training loss is 0.342679, validation loss is 0.489978.\n",
      "epoch 278: training loss is 0.342352, validation loss is 0.489858.\n",
      "epoch 279: training loss is 0.342026, validation loss is 0.489777.\n",
      "epoch 280: training loss is 0.341702, validation loss is 0.489698.\n",
      "epoch 281: training loss is 0.341380, validation loss is 0.489586.\n",
      "epoch 282: training loss is 0.341060, validation loss is 0.489503.\n",
      "epoch 283: training loss is 0.340741, validation loss is 0.489393.\n",
      "epoch 284: training loss is 0.340422, validation loss is 0.489198.\n",
      "epoch 285: training loss is 0.340109, validation loss is 0.489018.\n",
      "epoch 286: training loss is 0.339792, validation loss is 0.488989.\n",
      "epoch 287: training loss is 0.339480, validation loss is 0.488818.\n",
      "epoch 288: training loss is 0.339167, validation loss is 0.488746.\n",
      "epoch 289: training loss is 0.338857, validation loss is 0.488690.\n",
      "epoch 290: training loss is 0.338550, validation loss is 0.488488.\n",
      "epoch 291: training loss is 0.338241, validation loss is 0.488438.\n",
      "epoch 292: training loss is 0.337936, validation loss is 0.488404.\n",
      "epoch 293: training loss is 0.337631, validation loss is 0.488250.\n",
      "epoch 294: training loss is 0.337329, validation loss is 0.488148.\n",
      "epoch 295: training loss is 0.337027, validation loss is 0.488036.\n",
      "epoch 296: training loss is 0.336730, validation loss is 0.487853.\n",
      "epoch 297: training loss is 0.336430, validation loss is 0.487794.\n",
      "epoch 298: training loss is 0.336132, validation loss is 0.487728.\n",
      "epoch 299: training loss is 0.335837, validation loss is 0.487685.\n"
     ]
    }
   ],
   "source": [
    "# Under BOCN-TFIDF situation, training the model by using the best parameters\n",
    "weights_bocn_tfidf, tra_loss_bocn_tfidf, val_loss_bocn_tfidf = SGD(train_tfidf_mat_bocn, train_label, \n",
    "                                                                   dev_tfidf_mat_bocn, dev_label, \n",
    "                                                                   lr = best_parameter_bocn_tfidf[0], \n",
    "                                                                   alpha = best_parameter_bocn_tfidf[1],\n",
    "                                                                   epochs = best_parameter_bocn_tfidf[2],\n",
    "                                                                   tolerance = best_parameter_bocn_tfidf[3], \n",
    "                                                                   print_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the figure to check the training and validation loss during the training process\n",
    "\n",
    "plot_loss_history(tra_loss_bocn_tfidf, val_loss_bocn_tfidf, \"BOCN-TFIDF(fig-4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the loss-epoch figure for model of BOCN-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In figure 4, we could know, the model for BOCN-TFIDF is trained well.\n",
    "\n",
    "1. Because the validation loss line is becoming flat from declining and the final value of validation loss is low (about 0.47), the model is not underfitted.\n",
    "\n",
    "2. The difference between the final training loss and the final validation loss is small (about 0.17) and the validation loss line doen't have the tendency to rise. So the model isn't overfitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8325\n",
      "Precision: 0.8275862068965517\n",
      "Recall: 0.84\n",
      "F1-Score: 0.8337468982630273\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy, precision, recall, and f1_score to evaluate the model under BOCN-TFIDF situation\n",
    "\n",
    "preds_bocn_tfidf = predict_class(test_tfidf_mat_bocn, weights_bocn_tfidf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_label, preds_bocn_tfidf))\n",
    "print('Precision:', precision_score(test_label, preds_bocn_tfidf))\n",
    "print('Recall:', recall_score(test_label, preds_bocn_tfidf))\n",
    "print('F1-Score:', f1_score(test_label, preds_bocn_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "wors\n",
      "was\n",
      "orst\n",
      "ppose\n",
      "ppos\n",
      "poo\n",
      "adl\n",
      "nothi\n",
      "dio\n"
     ]
    }
   ],
   "source": [
    "# print top 10 words for negative class and positive class respectively.\n",
    "\n",
    "top_neg_bocn_tfidf = weights_bocn_tfidf.reshape(1, -1).argsort()[0][:10]\n",
    "for i in top_neg_bocn_tfidf:\n",
    "    print(id2word_bocn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grea\n",
      "great\n",
      "erful\n",
      "rful\n",
      "perfe\n",
      "rfec\n",
      "erfec\n",
      "rfect\n",
      "beau\n",
      "ilar\n"
     ]
    }
   ],
   "source": [
    "top_pos_bocn_tfidf = weights_bocn_tfidf.reshape(1, -1).argsort()[0][::-1][:10]\n",
    "for i in top_pos_bocn_tfidf:\n",
    "    print(id2word_bocn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the top-10 words for model of BOCN-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These character combinations ('bad', 'wors', 'orst', 'nothi', 'grea', 'great', 'perfe', 'rfec', 'erfec', rfect', 'beau', and 'ilar') make sense for classification. For example, 'wors', 'orst' are part of 'worst'; 'grea', 'great' are part of 'great'; 'perfe', 'rfec', 'erfec', 'rfect' are part of 'perfect'.\n",
    "\n",
    "Meanwhile, I think these aforementioned character combinations are generalized for the classification task in other domain. Because they are part of those words which could be used to describe all kinds of merchandises. For instance, you could always say \"this is a perfect food/phone/book(any kind of products)\" and also, you could always say \"this is the worst movies/clothes/car(any kind of products) I've ever seen\"\n",
    "\n",
    "There are also some other character combinations could be used to the classification task in new domain. Such as \"brill(part of brilliant)\", \"awf(part of awful)\", \"favor(part of favorite)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how I choose hyperparameters for model of BOCN-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I choose the hyperparameters (learning rate, alpha, epoch, tolerance) by testing the development(validation) dataset on several different set of hyperparameters. The set of hyperparameters which could help the model to achieve the highest validation accuracy and lowest validation loss are the best hyperparameters. I will randomly display several set of hyperparameters and the corresponding validation loss and validation accuracy.\n",
    "\n",
    "| lr | alpha |epoch  | tolerance | val_loss | val_accuracy |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 0.000010  | 0.100000 | 300  |  0.000100 | 0.503381  | 0.770000 |\n",
    "| 0.000010  |  0.500000| 300  |  0.000100  | 0.543069 | 0.780000 |\n",
    "| 0.000010  | 0.100000   |100  | 0.000100 | 0.536973 |0.780000 |\n",
    "| 0.000001 | 0.100000  |  300  | 0.000100  | 0.608475  | 0.735000  |\n",
    "| 0.000010  | 0.100000   |  300 | 0.000010  |0.489931 |0.765000 |\n",
    "\n",
    "From the above table, we could find when the lr = 1e-5, alpha = 0.1, epoch = 300, tolerance = 1e-5, the validation loss is lowest (0.4899). Therefore, I choose this set of hyperparameters as my best hyperparameters for model of BOCN-TFIDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5: BOW+BOCN(TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluate BOW + BOCN (TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary and vectorized feature matrices for BOW + BOCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     combine the vocabulary for BOW and the vocabulary for BOCN, if there are words appearing in both vocab_bow and \n",
    "# vocab_bocn, these words are only counted one time in vocabulary for BOW+BOCN.\n",
    "\n",
    "vocab_bb = set.union(vocab_bow, vocab_bocn)\n",
    "\n",
    "\n",
    "#    create a dictionary to store the terms' document frequencies for BOW+BOCN \n",
    "df_bb = df_bow.copy()\n",
    "\n",
    "for term in df_bocn:\n",
    "    if term not in df_bow:\n",
    "        df_bb[term] = df_bocn[term]\n",
    "    else:\n",
    "        if df_bb[term] >= df_bocn[term]:\n",
    "            continue\n",
    "        else:\n",
    "            df_bb[term] = df_bocn[term]\n",
    "\n",
    "\n",
    "#     create 2 dictionaries: (1) id2word_bb: keys = vocabulary ids, values = words (2) word2id_bb: keys = words, \n",
    "# values = vocabulary ids for BOW+BOCN\n",
    "\n",
    "id2word_bb = dict()\n",
    "word2id_bb = dict()\n",
    "\n",
    "for i in range(len(vocab_bb)):\n",
    "    id2word_bb[i] = list(vocab_bb)[i]\n",
    "    word2id_bb[list(vocab_bb)[i]] = i\n",
    "\n",
    "\n",
    "\n",
    "#    Combine the two extracted features words lists of BOCN and BOW for BOW+BOCN mode.\n",
    "\n",
    "train_extracted_ngrams_bb = []\n",
    "for i in range(1400):\n",
    "    train_extracted_ngrams_bb.append(train_extracted_ngrams.copy()[i] + train_extracted_ngrams_bocn.copy()[i])\n",
    "\n",
    "dev_extracted_ngrams_bb = []\n",
    "for i in range(200):\n",
    "    dev_extracted_ngrams_bb.append(dev_extracted_ngrams.copy()[i] + dev_extracted_ngrams_bocn.copy()[i])\n",
    "    \n",
    "test_extracted_ngrams_bb = []\n",
    "for i in range(400):\n",
    "    test_extracted_ngrams_bb.append(test_extracted_ngrams.copy()[i] + test_extracted_ngrams_bocn.copy()[i])\n",
    "\n",
    "\n",
    "#    vectorize the combined extracted features words for BOW+BOCN mode\n",
    "\n",
    "train_count_mat_bb = vectorise(train_extracted_ngrams_bb, vocab_bb, word2id_bb)\n",
    "dev_count_mat_bb = vectorise(dev_extracted_ngrams_bb, vocab_bb, word2id_bb)\n",
    "test_count_mat_bb = vectorise(test_extracted_ngrams_bb, vocab_bb, word2id_bb)\n",
    "\n",
    "\n",
    "#    obtain the tfidf features for BOW+BOCN mode\n",
    "\n",
    "# compute the inverted document frequencies values for words in vocab_bb\n",
    "idfs_bb = np.zeros((1, len(vocab_bb)))\n",
    "for i in range(len(vocab_bb)):\n",
    "    idfs_bb[0, i] = np.log10(len(train_text)/df_bb[list(vocab_bb)[i]])\n",
    "\n",
    "# transform the count vectors to tf.idf vectors\n",
    "\n",
    "train_tfidf_mat_bb = train_count_mat_bb * idfs_bb\n",
    "dev_tfidf_mat_bb = dev_count_mat_bb * idfs_bb\n",
    "test_tfidf_mat_bb = test_count_mat_bb * idfs_bb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tunning for the model of BOW+BOCN-TFIDF\n",
    "\n",
    "Please don't count the running time of this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.499285, val_acc = 0.800000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.499253, val_acc = 0.800000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.482736, val_acc = 0.815000\n",
      "\n",
      "lr = 0.000010, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.476746, val_acc = 0.815000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.473798, val_acc = 0.810000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.473762, val_acc = 0.805000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.426583, val_acc = 0.830000\n",
      "\n",
      "lr = 0.000010, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.422459, val_acc = 0.835000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000100, val_loss = 0.627164, val_acc = 0.750000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 100, tolerance = 0.000010, val_loss = 0.627156, val_acc = 0.750000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000100, val_loss = 0.564511, val_acc = 0.765000\n",
      "\n",
      "lr = 0.000001, alpha = 0.500000, epoch = 300, tolerance = 0.000010, val_loss = 0.564511, val_acc = 0.765000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000100, val_loss = 0.625623, val_acc = 0.750000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 100, tolerance = 0.000010, val_loss = 0.625627, val_acc = 0.750000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000100, val_loss = 0.557198, val_acc = 0.765000\n",
      "\n",
      "lr = 0.000001, alpha = 0.100000, epoch = 300, tolerance = 0.000010, val_loss = 0.557196, val_acc = 0.765000\n",
      "\n",
      "The best parameters are: lr = 1e-05, alpha = 0.1, epoch = 300, tolerance = 1e-05\n"
     ]
    }
   ],
   "source": [
    "# choose the best hyperparameters for BOW+BOCN-TFIDF\n",
    "\n",
    "best_parameter_bb_tfidf = choose_hyper(train_tfidf_mat_bb, train_label, dev_tfidf_mat_bb, dev_label, \n",
    "                              lr_candidate = [1e-5, 1e-6], alpha_candidate = [5*1e-1, 1e-1], \n",
    "                              epoch_candidate = [100, 300], tolerance_candidate = [1e-4, 1e-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of the hyperparameters tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model for BOW+BOCN-TFIDF by using the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: training loss is 0.677986, validation loss is 0.682771.\n",
      "epoch   1: training loss is 0.664918, validation loss is 0.674573.\n",
      "epoch   2: training loss is 0.652740, validation loss is 0.667154.\n",
      "epoch   3: training loss is 0.641209, validation loss is 0.660362.\n",
      "epoch   4: training loss is 0.630339, validation loss is 0.653612.\n",
      "epoch   5: training loss is 0.619945, validation loss is 0.647576.\n",
      "epoch   6: training loss is 0.610114, validation loss is 0.641618.\n",
      "epoch   7: training loss is 0.600799, validation loss is 0.636198.\n",
      "epoch   8: training loss is 0.591912, validation loss is 0.630902.\n",
      "epoch   9: training loss is 0.583342, validation loss is 0.625526.\n",
      "epoch  10: training loss is 0.575220, validation loss is 0.620630.\n",
      "epoch  11: training loss is 0.567518, validation loss is 0.615830.\n",
      "epoch  12: training loss is 0.560103, validation loss is 0.611365.\n",
      "epoch  13: training loss is 0.552899, validation loss is 0.607137.\n",
      "epoch  14: training loss is 0.546020, validation loss is 0.603100.\n",
      "epoch  15: training loss is 0.539438, validation loss is 0.599146.\n",
      "epoch  16: training loss is 0.533094, validation loss is 0.595461.\n",
      "epoch  17: training loss is 0.526982, validation loss is 0.591791.\n",
      "epoch  18: training loss is 0.521096, validation loss is 0.588272.\n",
      "epoch  19: training loss is 0.515415, validation loss is 0.584910.\n",
      "epoch  20: training loss is 0.509929, validation loss is 0.581749.\n",
      "epoch  21: training loss is 0.504617, validation loss is 0.578569.\n",
      "epoch  22: training loss is 0.499512, validation loss is 0.575508.\n",
      "epoch  23: training loss is 0.494510, validation loss is 0.572647.\n",
      "epoch  24: training loss is 0.489721, validation loss is 0.569934.\n",
      "epoch  25: training loss is 0.485029, validation loss is 0.567150.\n",
      "epoch  26: training loss is 0.480516, validation loss is 0.564561.\n",
      "epoch  27: training loss is 0.476089, validation loss is 0.561939.\n",
      "epoch  28: training loss is 0.471813, validation loss is 0.559421.\n",
      "epoch  29: training loss is 0.467653, validation loss is 0.557023.\n",
      "epoch  30: training loss is 0.463604, validation loss is 0.554704.\n",
      "epoch  31: training loss is 0.459680, validation loss is 0.552431.\n",
      "epoch  32: training loss is 0.455835, validation loss is 0.550238.\n",
      "epoch  33: training loss is 0.452097, validation loss is 0.548106.\n",
      "epoch  34: training loss is 0.448487, validation loss is 0.546039.\n",
      "epoch  35: training loss is 0.444909, validation loss is 0.544008.\n",
      "epoch  36: training loss is 0.441438, validation loss is 0.542043.\n",
      "epoch  37: training loss is 0.438056, validation loss is 0.540122.\n",
      "epoch  38: training loss is 0.434751, validation loss is 0.538257.\n",
      "epoch  39: training loss is 0.431539, validation loss is 0.536457.\n",
      "epoch  40: training loss is 0.428376, validation loss is 0.534669.\n",
      "epoch  41: training loss is 0.425287, validation loss is 0.532930.\n",
      "epoch  42: training loss is 0.422279, validation loss is 0.531254.\n",
      "epoch  43: training loss is 0.419323, validation loss is 0.529596.\n",
      "epoch  44: training loss is 0.416438, validation loss is 0.527987.\n",
      "epoch  45: training loss is 0.413604, validation loss is 0.526400.\n",
      "epoch  46: training loss is 0.410837, validation loss is 0.524871.\n",
      "epoch  47: training loss is 0.408134, validation loss is 0.523385.\n",
      "epoch  48: training loss is 0.405461, validation loss is 0.521886.\n",
      "epoch  49: training loss is 0.402864, validation loss is 0.520438.\n",
      "epoch  50: training loss is 0.400308, validation loss is 0.519027.\n",
      "epoch  51: training loss is 0.397793, validation loss is 0.517651.\n",
      "epoch  52: training loss is 0.395333, validation loss is 0.516298.\n",
      "epoch  53: training loss is 0.392922, validation loss is 0.514973.\n",
      "epoch  54: training loss is 0.390550, validation loss is 0.513688.\n",
      "epoch  55: training loss is 0.388226, validation loss is 0.512428.\n",
      "epoch  56: training loss is 0.385941, validation loss is 0.511176.\n",
      "epoch  57: training loss is 0.383697, validation loss is 0.509948.\n",
      "epoch  58: training loss is 0.381492, validation loss is 0.508753.\n",
      "epoch  59: training loss is 0.379327, validation loss is 0.507578.\n",
      "epoch  60: training loss is 0.377206, validation loss is 0.506406.\n",
      "epoch  61: training loss is 0.375123, validation loss is 0.505268.\n",
      "epoch  62: training loss is 0.373048, validation loss is 0.504190.\n",
      "epoch  63: training loss is 0.371025, validation loss is 0.503112.\n",
      "epoch  64: training loss is 0.369034, validation loss is 0.502044.\n",
      "epoch  65: training loss is 0.367076, validation loss is 0.500973.\n",
      "epoch  66: training loss is 0.365148, validation loss is 0.499951.\n",
      "epoch  67: training loss is 0.363252, validation loss is 0.498943.\n",
      "epoch  68: training loss is 0.361387, validation loss is 0.497925.\n",
      "epoch  69: training loss is 0.359547, validation loss is 0.496952.\n",
      "epoch  70: training loss is 0.357736, validation loss is 0.496009.\n",
      "epoch  71: training loss is 0.355959, validation loss is 0.495098.\n",
      "epoch  72: training loss is 0.354199, validation loss is 0.494138.\n",
      "epoch  73: training loss is 0.352470, validation loss is 0.493199.\n",
      "epoch  74: training loss is 0.350766, validation loss is 0.492327.\n",
      "epoch  75: training loss is 0.349086, validation loss is 0.491420.\n",
      "epoch  76: training loss is 0.347430, validation loss is 0.490563.\n",
      "epoch  77: training loss is 0.345799, validation loss is 0.489699.\n",
      "epoch  78: training loss is 0.344191, validation loss is 0.488850.\n",
      "epoch  79: training loss is 0.342607, validation loss is 0.487997.\n",
      "epoch  80: training loss is 0.341041, validation loss is 0.487206.\n",
      "epoch  81: training loss is 0.339500, validation loss is 0.486387.\n",
      "epoch  82: training loss is 0.337978, validation loss is 0.485620.\n",
      "epoch  83: training loss is 0.336478, validation loss is 0.484816.\n",
      "epoch  84: training loss is 0.334998, validation loss is 0.484080.\n",
      "epoch  85: training loss is 0.333537, validation loss is 0.483321.\n",
      "epoch  86: training loss is 0.332095, validation loss is 0.482565.\n",
      "epoch  87: training loss is 0.330694, validation loss is 0.481954.\n",
      "epoch  88: training loss is 0.329281, validation loss is 0.481196.\n",
      "epoch  89: training loss is 0.327884, validation loss is 0.480412.\n",
      "epoch  90: training loss is 0.326515, validation loss is 0.479700.\n",
      "epoch  91: training loss is 0.325164, validation loss is 0.478997.\n",
      "epoch  92: training loss is 0.323830, validation loss is 0.478331.\n",
      "epoch  93: training loss is 0.322512, validation loss is 0.477628.\n",
      "epoch  94: training loss is 0.321212, validation loss is 0.476934.\n",
      "epoch  95: training loss is 0.319927, validation loss is 0.476281.\n",
      "epoch  96: training loss is 0.318656, validation loss is 0.475684.\n",
      "epoch  97: training loss is 0.317401, validation loss is 0.475015.\n",
      "epoch  98: training loss is 0.316161, validation loss is 0.474401.\n",
      "epoch  99: training loss is 0.314937, validation loss is 0.473756.\n",
      "epoch 100: training loss is 0.313730, validation loss is 0.473211.\n",
      "epoch 101: training loss is 0.312533, validation loss is 0.472516.\n",
      "epoch 102: training loss is 0.311349, validation loss is 0.471972.\n",
      "epoch 103: training loss is 0.310181, validation loss is 0.471389.\n",
      "epoch 104: training loss is 0.309026, validation loss is 0.470782.\n",
      "epoch 105: training loss is 0.307884, validation loss is 0.470231.\n",
      "epoch 106: training loss is 0.306755, validation loss is 0.469649.\n",
      "epoch 107: training loss is 0.305640, validation loss is 0.469126.\n",
      "epoch 108: training loss is 0.304535, validation loss is 0.468541.\n",
      "epoch 109: training loss is 0.303445, validation loss is 0.467966.\n",
      "epoch 110: training loss is 0.302365, validation loss is 0.467477.\n",
      "epoch 111: training loss is 0.301297, validation loss is 0.466933.\n",
      "epoch 112: training loss is 0.300241, validation loss is 0.466423.\n",
      "epoch 113: training loss is 0.299196, validation loss is 0.465881.\n",
      "epoch 114: training loss is 0.298163, validation loss is 0.465375.\n",
      "epoch 115: training loss is 0.297141, validation loss is 0.464893.\n",
      "epoch 116: training loss is 0.296130, validation loss is 0.464340.\n",
      "epoch 117: training loss is 0.295129, validation loss is 0.463878.\n",
      "epoch 118: training loss is 0.294139, validation loss is 0.463378.\n",
      "epoch 119: training loss is 0.293159, validation loss is 0.462920.\n",
      "epoch 120: training loss is 0.292189, validation loss is 0.462435.\n",
      "epoch 121: training loss is 0.291229, validation loss is 0.461951.\n",
      "epoch 122: training loss is 0.290279, validation loss is 0.461522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123: training loss is 0.289342, validation loss is 0.461106.\n",
      "epoch 124: training loss is 0.288408, validation loss is 0.460591.\n",
      "epoch 125: training loss is 0.287486, validation loss is 0.460126.\n",
      "epoch 126: training loss is 0.286574, validation loss is 0.459716.\n",
      "epoch 127: training loss is 0.285676, validation loss is 0.459335.\n",
      "epoch 128: training loss is 0.284777, validation loss is 0.458852.\n",
      "epoch 129: training loss is 0.283890, validation loss is 0.458381.\n",
      "epoch 130: training loss is 0.283015, validation loss is 0.458011.\n",
      "epoch 131: training loss is 0.282148, validation loss is 0.457614.\n",
      "epoch 132: training loss is 0.281287, validation loss is 0.457185.\n",
      "epoch 133: training loss is 0.280434, validation loss is 0.456756.\n",
      "epoch 134: training loss is 0.279590, validation loss is 0.456339.\n",
      "epoch 135: training loss is 0.278754, validation loss is 0.455925.\n",
      "epoch 136: training loss is 0.277926, validation loss is 0.455519.\n",
      "epoch 137: training loss is 0.277106, validation loss is 0.455140.\n",
      "epoch 138: training loss is 0.276293, validation loss is 0.454736.\n",
      "epoch 139: training loss is 0.275489, validation loss is 0.454383.\n",
      "epoch 140: training loss is 0.274692, validation loss is 0.453932.\n",
      "epoch 141: training loss is 0.273901, validation loss is 0.453575.\n",
      "epoch 142: training loss is 0.273119, validation loss is 0.453234.\n",
      "epoch 143: training loss is 0.272344, validation loss is 0.452818.\n",
      "epoch 144: training loss is 0.271575, validation loss is 0.452468.\n",
      "epoch 145: training loss is 0.270813, validation loss is 0.452136.\n",
      "epoch 146: training loss is 0.270059, validation loss is 0.451825.\n",
      "epoch 147: training loss is 0.269310, validation loss is 0.451462.\n",
      "epoch 148: training loss is 0.268568, validation loss is 0.451068.\n",
      "epoch 149: training loss is 0.267835, validation loss is 0.450689.\n",
      "epoch 150: training loss is 0.267105, validation loss is 0.450397.\n",
      "epoch 151: training loss is 0.266383, validation loss is 0.450084.\n",
      "epoch 152: training loss is 0.265667, validation loss is 0.449730.\n",
      "epoch 153: training loss is 0.264958, validation loss is 0.449402.\n",
      "epoch 154: training loss is 0.264254, validation loss is 0.449085.\n",
      "epoch 155: training loss is 0.263557, validation loss is 0.448731.\n",
      "epoch 156: training loss is 0.262867, validation loss is 0.448469.\n",
      "epoch 157: training loss is 0.262180, validation loss is 0.448114.\n",
      "epoch 158: training loss is 0.261501, validation loss is 0.447820.\n",
      "epoch 159: training loss is 0.260826, validation loss is 0.447469.\n",
      "epoch 160: training loss is 0.260158, validation loss is 0.447138.\n",
      "epoch 161: training loss is 0.259496, validation loss is 0.446833.\n",
      "epoch 162: training loss is 0.258838, validation loss is 0.446544.\n",
      "epoch 163: training loss is 0.258187, validation loss is 0.446224.\n",
      "epoch 164: training loss is 0.257540, validation loss is 0.445977.\n",
      "epoch 165: training loss is 0.256899, validation loss is 0.445683.\n",
      "epoch 166: training loss is 0.256264, validation loss is 0.445349.\n",
      "epoch 167: training loss is 0.255633, validation loss is 0.445054.\n",
      "epoch 168: training loss is 0.255008, validation loss is 0.444846.\n",
      "epoch 169: training loss is 0.254387, validation loss is 0.444558.\n",
      "epoch 170: training loss is 0.253772, validation loss is 0.444260.\n",
      "epoch 171: training loss is 0.253163, validation loss is 0.444035.\n",
      "epoch 172: training loss is 0.252560, validation loss is 0.443785.\n",
      "epoch 173: training loss is 0.251957, validation loss is 0.443482.\n",
      "epoch 174: training loss is 0.251359, validation loss is 0.443179.\n",
      "epoch 175: training loss is 0.250768, validation loss is 0.442913.\n",
      "epoch 176: training loss is 0.250181, validation loss is 0.442633.\n",
      "epoch 177: training loss is 0.249600, validation loss is 0.442418.\n",
      "epoch 178: training loss is 0.249021, validation loss is 0.442129.\n",
      "epoch 179: training loss is 0.248449, validation loss is 0.441884.\n",
      "epoch 180: training loss is 0.247884, validation loss is 0.441691.\n",
      "epoch 181: training loss is 0.247317, validation loss is 0.441405.\n",
      "epoch 182: training loss is 0.246758, validation loss is 0.441168.\n",
      "epoch 183: training loss is 0.246200, validation loss is 0.440850.\n",
      "epoch 184: training loss is 0.245649, validation loss is 0.440626.\n",
      "epoch 185: training loss is 0.245101, validation loss is 0.440369.\n",
      "epoch 186: training loss is 0.244560, validation loss is 0.440172.\n",
      "epoch 187: training loss is 0.244020, validation loss is 0.439924.\n",
      "epoch 188: training loss is 0.243485, validation loss is 0.439672.\n",
      "epoch 189: training loss is 0.242954, validation loss is 0.439429.\n",
      "epoch 190: training loss is 0.242427, validation loss is 0.439158.\n",
      "epoch 191: training loss is 0.241904, validation loss is 0.438933.\n",
      "epoch 192: training loss is 0.241384, validation loss is 0.438700.\n",
      "epoch 193: training loss is 0.240869, validation loss is 0.438486.\n",
      "epoch 194: training loss is 0.240358, validation loss is 0.438241.\n",
      "epoch 195: training loss is 0.239851, validation loss is 0.437999.\n",
      "epoch 196: training loss is 0.239351, validation loss is 0.437725.\n",
      "epoch 197: training loss is 0.238848, validation loss is 0.437537.\n",
      "epoch 198: training loss is 0.238349, validation loss is 0.437349.\n",
      "epoch 199: training loss is 0.237856, validation loss is 0.437140.\n",
      "epoch 200: training loss is 0.237367, validation loss is 0.436908.\n",
      "epoch 201: training loss is 0.236880, validation loss is 0.436730.\n",
      "epoch 202: training loss is 0.236397, validation loss is 0.436569.\n",
      "epoch 203: training loss is 0.235918, validation loss is 0.436366.\n",
      "epoch 204: training loss is 0.235442, validation loss is 0.436121.\n",
      "epoch 205: training loss is 0.234970, validation loss is 0.435972.\n",
      "epoch 206: training loss is 0.234500, validation loss is 0.435747.\n",
      "epoch 207: training loss is 0.234037, validation loss is 0.435600.\n",
      "epoch 208: training loss is 0.233572, validation loss is 0.435364.\n",
      "epoch 209: training loss is 0.233113, validation loss is 0.435164.\n",
      "epoch 210: training loss is 0.232657, validation loss is 0.434976.\n",
      "epoch 211: training loss is 0.232204, validation loss is 0.434787.\n",
      "epoch 212: training loss is 0.231755, validation loss is 0.434609.\n",
      "epoch 213: training loss is 0.231308, validation loss is 0.434420.\n",
      "epoch 214: training loss is 0.230863, validation loss is 0.434194.\n",
      "epoch 215: training loss is 0.230423, validation loss is 0.434016.\n",
      "epoch 216: training loss is 0.229985, validation loss is 0.433799.\n",
      "epoch 217: training loss is 0.229550, validation loss is 0.433635.\n",
      "epoch 218: training loss is 0.229118, validation loss is 0.433451.\n",
      "epoch 219: training loss is 0.228691, validation loss is 0.433316.\n",
      "epoch 220: training loss is 0.228263, validation loss is 0.433084.\n",
      "epoch 221: training loss is 0.227840, validation loss is 0.432885.\n",
      "epoch 222: training loss is 0.227421, validation loss is 0.432741.\n",
      "epoch 223: training loss is 0.227003, validation loss is 0.432556.\n",
      "epoch 224: training loss is 0.226588, validation loss is 0.432375.\n",
      "epoch 225: training loss is 0.226176, validation loss is 0.432194.\n",
      "epoch 226: training loss is 0.225767, validation loss is 0.432047.\n",
      "epoch 227: training loss is 0.225361, validation loss is 0.431863.\n",
      "epoch 228: training loss is 0.224958, validation loss is 0.431641.\n",
      "epoch 229: training loss is 0.224556, validation loss is 0.431479.\n",
      "epoch 230: training loss is 0.224158, validation loss is 0.431300.\n",
      "epoch 231: training loss is 0.223762, validation loss is 0.431158.\n",
      "epoch 232: training loss is 0.223368, validation loss is 0.431015.\n",
      "epoch 233: training loss is 0.222978, validation loss is 0.430909.\n",
      "epoch 234: training loss is 0.222589, validation loss is 0.430717.\n",
      "epoch 235: training loss is 0.222203, validation loss is 0.430518.\n",
      "epoch 236: training loss is 0.221820, validation loss is 0.430390.\n",
      "epoch 237: training loss is 0.221439, validation loss is 0.430236.\n",
      "epoch 238: training loss is 0.221061, validation loss is 0.430100.\n",
      "epoch 239: training loss is 0.220685, validation loss is 0.429943.\n",
      "epoch 240: training loss is 0.220311, validation loss is 0.429767.\n",
      "epoch 241: training loss is 0.219940, validation loss is 0.429638.\n",
      "epoch 242: training loss is 0.219572, validation loss is 0.429417.\n",
      "epoch 243: training loss is 0.219205, validation loss is 0.429297.\n",
      "epoch 244: training loss is 0.218841, validation loss is 0.429147.\n",
      "epoch 245: training loss is 0.218479, validation loss is 0.429002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 246: training loss is 0.218119, validation loss is 0.428862.\n",
      "epoch 247: training loss is 0.217762, validation loss is 0.428730.\n",
      "epoch 248: training loss is 0.217407, validation loss is 0.428601.\n",
      "epoch 249: training loss is 0.217054, validation loss is 0.428403.\n",
      "epoch 250: training loss is 0.216704, validation loss is 0.428252.\n",
      "epoch 251: training loss is 0.216354, validation loss is 0.428157.\n",
      "epoch 252: training loss is 0.216008, validation loss is 0.428028.\n",
      "epoch 253: training loss is 0.215664, validation loss is 0.427920.\n",
      "epoch 254: training loss is 0.215322, validation loss is 0.427800.\n",
      "epoch 255: training loss is 0.214981, validation loss is 0.427642.\n",
      "epoch 256: training loss is 0.214643, validation loss is 0.427489.\n",
      "epoch 257: training loss is 0.214307, validation loss is 0.427328.\n",
      "epoch 258: training loss is 0.213973, validation loss is 0.427191.\n",
      "epoch 259: training loss is 0.213642, validation loss is 0.427106.\n",
      "epoch 260: training loss is 0.213311, validation loss is 0.426942.\n",
      "epoch 261: training loss is 0.212983, validation loss is 0.426814.\n",
      "epoch 262: training loss is 0.212657, validation loss is 0.426680.\n",
      "epoch 263: training loss is 0.212334, validation loss is 0.426532.\n",
      "epoch 264: training loss is 0.212011, validation loss is 0.426413.\n",
      "epoch 265: training loss is 0.211691, validation loss is 0.426300.\n",
      "epoch 266: training loss is 0.211374, validation loss is 0.426227.\n",
      "epoch 267: training loss is 0.211057, validation loss is 0.426049.\n",
      "epoch 268: training loss is 0.210742, validation loss is 0.425942.\n",
      "epoch 269: training loss is 0.210430, validation loss is 0.425844.\n",
      "epoch 270: training loss is 0.210119, validation loss is 0.425693.\n",
      "epoch 271: training loss is 0.209810, validation loss is 0.425593.\n",
      "epoch 272: training loss is 0.209503, validation loss is 0.425470.\n",
      "epoch 273: training loss is 0.209198, validation loss is 0.425376.\n",
      "epoch 274: training loss is 0.208894, validation loss is 0.425219.\n",
      "epoch 275: training loss is 0.208592, validation loss is 0.425116.\n",
      "epoch 276: training loss is 0.208292, validation loss is 0.424997.\n",
      "epoch 277: training loss is 0.207994, validation loss is 0.424861.\n",
      "epoch 278: training loss is 0.207697, validation loss is 0.424741.\n",
      "epoch 279: training loss is 0.207402, validation loss is 0.424652.\n",
      "epoch 280: training loss is 0.207109, validation loss is 0.424520.\n",
      "epoch 281: training loss is 0.206818, validation loss is 0.424424.\n",
      "epoch 282: training loss is 0.206528, validation loss is 0.424291.\n",
      "epoch 283: training loss is 0.206240, validation loss is 0.424197.\n",
      "epoch 284: training loss is 0.205953, validation loss is 0.424102.\n",
      "epoch 285: training loss is 0.205668, validation loss is 0.423945.\n",
      "epoch 286: training loss is 0.205385, validation loss is 0.423845.\n",
      "epoch 287: training loss is 0.205104, validation loss is 0.423727.\n",
      "epoch 288: training loss is 0.204825, validation loss is 0.423566.\n",
      "epoch 289: training loss is 0.204545, validation loss is 0.423508.\n",
      "epoch 290: training loss is 0.204268, validation loss is 0.423441.\n",
      "epoch 291: training loss is 0.203993, validation loss is 0.423356.\n",
      "epoch 292: training loss is 0.203719, validation loss is 0.423244.\n",
      "epoch 293: training loss is 0.203447, validation loss is 0.423152.\n",
      "epoch 294: training loss is 0.203176, validation loss is 0.423038.\n",
      "epoch 295: training loss is 0.202907, validation loss is 0.422931.\n",
      "epoch 296: training loss is 0.202639, validation loss is 0.422823.\n",
      "epoch 297: training loss is 0.202373, validation loss is 0.422694.\n",
      "epoch 298: training loss is 0.202109, validation loss is 0.422570.\n",
      "epoch 299: training loss is 0.201845, validation loss is 0.422474.\n"
     ]
    }
   ],
   "source": [
    "# Under BOW+BOCN-TFIDF situation, training the model by using the best parameters\n",
    "weights_bb_tfidf, tra_loss_bb_tfidf, val_loss_bb_tfidf = SGD(train_tfidf_mat_bb, train_label, \n",
    "                                                             dev_tfidf_mat_bb, dev_label, \n",
    "                                                             lr = best_parameter_bb_tfidf[0], \n",
    "                                                             alpha = best_parameter_bb_tfidf[1],\n",
    "                                                             epochs = best_parameter_bb_tfidf[2],\n",
    "                                                             tolerance = best_parameter_bb_tfidf[3], \n",
    "                                                             print_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the figure to check the training and validation loss during the training process\n",
    "\n",
    "plot_loss_history(tra_loss_bb_tfidf, val_loss_bb_tfidf, \"BOW+BOCN-TFIDF(fig-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the loss-epoch figure for model of BOW+BOCN-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From figure 5, we could find the model for BOW+BOCN-TFIDF is trained well.\n",
    "\n",
    "1. The final test accuracy is high enough (86%), the final validation loss is low enough(about 0.42), and the validation loss line begin to be flat from declining. So the model is not underfitted.\n",
    "\n",
    "2. The difference between the final training loss and the final validation loss is small (about 0.22), and the validation loss line doesn't have tendency to rise at the end of model training. Therefore the model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "Precision: 0.8461538461538461\n",
      "Recall: 0.88\n",
      "F1-Score: 0.8627450980392156\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy, precision, recall, and f1_score to evaluate the model under BOCN-TFIDF situation\n",
    "\n",
    "preds_bb_tfidf = predict_class(test_tfidf_mat_bb, weights_bb_tfidf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_label, preds_bb_tfidf))\n",
    "print('Precision:', precision_score(test_label, preds_bb_tfidf))\n",
    "print('Recall:', recall_score(test_label, preds_bb_tfidf))\n",
    "print('F1-Score:', f1_score(test_label, preds_bb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "worst\n",
      "waste\n",
      "poor\n",
      "dull\n",
      "awful\n",
      "lame\n",
      "fails\n",
      "plot\n",
      "looks\n"
     ]
    }
   ],
   "source": [
    "# print top 10 words for negative class and positive class respectively.\n",
    "\n",
    "top_neg_bb_tfidf = weights_bb_tfidf.reshape(1, -1).argsort()[0][:10]\n",
    "for i in top_neg_bb_tfidf:\n",
    "    print(id2word_bb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "pulp\n",
      "quite\n",
      "true\n",
      "beau\n",
      "life\n",
      "seen\n",
      "world\n",
      "damon\n",
      "rush\n"
     ]
    }
   ],
   "source": [
    "top_pos_bb_tfidf = weights_bb_tfidf.reshape(1, -1).argsort()[0][::-1][:10]\n",
    "for i in top_pos_bb_tfidf:\n",
    "    print(id2word_bb[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the top-10 words for model of BOW+BOCN-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-10 words and character combinations of the model for BOW+BOCN-TFIDF make sense for the sentimental classification task.\n",
    "\n",
    "We could easily find the words \"bad\", \"worst\", \"poor\", \"dull\", \"awful\" could be used to describe movies with a negative emotion. Then the words \"great\", \"beau\", could be used to describe movies with positive emotion.\n",
    "\n",
    "Besides, these words are generalized well for other domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how I choose hyperparameters for model of BOW+BOCN-TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I choose the hyperparameters (learning rate, alpha, epoch, tolerance) by testing the development(validation) dataset on several different set of hyperparameters. The set of hyperparameters which could help the model to achieve the highest validation accuracy and lowest validation loss are the best hyperparameters. I will randomly display several set of hyperparameters and the corresponding validation loss and validation accuracy.\n",
    "\n",
    "| lr | alpha |epoch  | tolerance | val_loss | val_accuracy |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 0.000010  | 0.100000 | 300  |  0.000100 | 0.426583  | 0.830000 |\n",
    "| 0.000010  |  0.500000| 300  |  0.000100  | 0.482736 | 0.815000 |\n",
    "| 0.000010  | 0.100000   |100  | 0.000100 | 0.473798 |0.810000 |\n",
    "| 0.000001 | 0.100000  |  300  | 0.000100  | 0.557198  | 0.765000 |\n",
    "| 0.000010  | 0.100000   |  300 | 0.000010  |0.422459 |0.835000 |\n",
    "\n",
    "When lr = 1e-5, alpha = 0.1, epoch = 300, tolerance = 1e-5, the model could have the lowest validation loss. So this set of hyperparameters is my best hyperparameters for model of BOW+BOCN-TIFDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Accuracy |Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |  0.8225 | 0.8057  |  0.85 | 0.8273  |\n",
    "| BOW-tfidf  |  0.865 | 0.851  | 0.885  |  0.8676 |\n",
    "| BOCN-count  | 0.82  | 0.8232  | 0.815  | 0.8191  |\n",
    "| BOCN-tfidf  | 0.8325  |  0.8276 | 0.84  | 0.8337  |\n",
    "| BOW+BOCN-tfidf  | 0.86  |  0.8462 | 0.88  | 0.8627  |\n",
    "\n",
    "Please discuss why your best performing model is better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, we could find my best model is the model for BOW-tfidf. The model for BOW-tfidf has a 0.8676 F1-score. Then, the BOW+BOCN-tfidf model's F1-score (0.8627) is slightly lower than BOW-tfidf's. \n",
    "\n",
    "First, I think the reason for why BOW-tfidf model is better than BOW-count model is that BOW-tfidf uses tfidf weighted feature values. Compared with raw-frequency weighted feature values, tfidf will help model classify which words are more informative. \n",
    "\n",
    "For example, if I input a sentence \"Sheffield is a beautiful city\" to a raw-frequency based model, the model will treat all five words in this sentence equally. It means, for the model, the word \"a\" contains same information with the word \"beautiful\". In fact, it is obvious for human that the word \"beautiful\" contains more information than \"a\" and \"is\" to help us know this sentence is a positive comment. Usually, this kind of words with low information such as \"a\", \"the\", \"is\" would appear lots of times in the whole document collection. It means the informativeness is inveresly related to document frequency(df). The less common words (low df value) are usually more informative. Therefore, we could use tfidf weighted method to give those words with low information a low weight and give those words with high information a high weight.\n",
    "\n",
    "By using TFIDF weighted method, the words with low document frequencies but with high information could influence more in the text classification task. Therefore, the BOW-tfidf has a better performance than BOW-count.\n",
    "\n",
    "Second, compared with BOCN models, the BOW has a better performance. I think the reason might be the BOCN model splits the key words for classification and because the capacity of vocab_bocn(vocabulary for BOCN) is limited, the vocab_bocn contains less key words than vocab_bow(vocabulary for BOW). For example, we could see from the top-10 positive words of BOCN-tfidf model, one key word \"great\" is splited to \"gre\", \"grea\", \"great\", and one key word \"perfectly\" is splited to \"perf\" \"rfect\", \"erfec\". In this way, if vocab_bow contains 4000 different key words, the vocab_bocn could only contain 4000 character combinations from 1000 different key words. I think this will lead the vocab_bocn to be low informative. So the BOW has better performance than BOCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion about the choices of text processing and hyperparameters values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. About the choices of text processing, I have commented in the coding. Let me do a summary here.\n",
    "\n",
    "1.1 First, to process the original text in documents, I tried to make all letters in text be lowercase (but I found this task had been done). Then, I removed all numbers and punctuations from the texts because these characters are not useful for sentimental classification. \n",
    "\n",
    "After this, I find all words with two or more than two letters. It means the simple words such as \"a\", \"I\" are dropped. Then, based on the flag char_ngrams, my extract_ngrams function could extract words or character combinations for each document in document collection.\n",
    "\n",
    "1.2 Then, I adjusted the stoplist. I added 'am', 'll', 'also', 'why', 'would', 'should', 'their', 'might', 'there', 'where', 'him' to the original stoplist because I think these words have a high term frequencies in the whole collection but with low information to help the model do classification.\n",
    "\n",
    "1.3 Thirdly, about the vocabulary size, it should be splited to two part. The first one is vocab_bow (vocabulary for the model of BOW) and the second one is vocab_bocn (vocabulary for the model of BOCN). For the vocab_bow, I only picked those words with a document frequency higher than 20. At this time, the vocabulary size of vocab_bow is 4488. For the vocab_bocn, I set the keep_topN = 4000. It means to pick top 4000 of term frequency in collection of character combinations and therefore, the size of vocab_bocn is 4000. Then, in the training process, I found the vocabulary with size of 4000 provided enough information for model training. All the models are trained well with 83% test accuracy in average and the training time is fast. Therefore, I think 4000 is a proper vocabulary size. \n",
    "\n",
    "1.4 Then, about the ngram_range, I choose to (1, 3) for BOW model and (3, 5) for BOCN model. The reason for why I choose (3, 5) as ngram_range for BOCN model is:\n",
    "\n",
    "If the ngram_range value is too small, the character combinations will be too short. At this time, different words could produce the same character combinations. For example, a negative word \"worst\" contain character combinations \"wo\", but a positive word \"wonderful\" could also contain a character combinations \"wo\". So, if the ngram_range value is too small, the short character combinations would be confusing.\n",
    "\n",
    "Similarly, if ngram_range is too large, the character combinations would be too long. At that time, a long character combination might only appear in one document. It will be also bad for model training.\n",
    "\n",
    "\n",
    "2. About the choices of hyperparameters.\n",
    "\n",
    "For all five models, I choose to use a low learning rate and a relatively high epoch number. Because I found when I use a high learning rate, although it could complete the training in a few epochs, the validation loss could not decline stablely (the validation loss may decline in this epoch and then, rise in next epoch, the validation loss line is fluctuant). So, I decided to use a small learning rate such as 1e-5 and a high epoch like 300. With this small learning rate and high epoch, the validation loss of my models could decline stablely and the trained model could trained well with a low validation loss.\n",
    "\n",
    "For the regularization hyperparameter alpha, I set it from 0.1 - 0.5. Because when I set it small such as 0.01 or 0.001, I found my model is often overfitted. The final training loss will be very small (lower than 0.1) but the validation loss is still higher than 0.3. Therefore, I set a relatively high alpha to avoid my models being overtrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
